{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2290c2b9",
   "metadata": {},
   "source": [
    "## 4 신경망 시작하기: 분류와 회귀 \n",
    "\n",
    "1 영화리뷰분류: 이진 분류 문제 <br>\n",
    "2 뉴스 기사 분류: 다중 분류 문제 <br>\n",
    "3 주택 가격 예측: 회귀 문제 <br>\n",
    "4 요약 <br>\n",
    "\n",
    "\n",
    "### 분류와 회귀에서 사용하는 용어 \n",
    "\n",
    "- <b>샘플</b> 또는 <b>입력</b>: 모델에 주입될 하나의 데이터 포인트(data point) <br>\n",
    "- <b>예측</b> 또는 <b>출력</b>: 모델로부터 나오는 값 <br>\n",
    "- <b>타깃</b>: 정답. 외부 데이터 소스에 근거하여 모델이 완벽하게 예측해야 하는 값 <br>\n",
    "- <b>예측 오차</b> 또는 <b> 손실 값</b>: 모델의 예측과 타깃 사이의 거리를 측정한 값 <br>\n",
    "- <b>클래스</b>: 분류 문제에서 선택할 수 있는 가능한 레이블의 집합. ex, 모델에 주입될 하나의 데이터 포인트(data point) <br>\n",
    "- <b>레이블</b>: 분류 문제에서 클래스 할당의 구체적인 사례.ex, #1234에 '강아지' 클래스가 들어 있다고 표시한다면 '강아지'는 사진 #1234의 레이블이 된다 <br>\n",
    "- <b>참값(ground-truth)</b> 또는 <b>애너테이션(annotation)</b>: 데이터셋이 대한 모든 타깃. 일반적으로 사람에 의해 수집됨 <br>\n",
    "- <b>이진 분류</b>: 각 입력 샘플이 2개의 배타적인 범주로 구분되는 분류 작업 <br>\n",
    "- <b>다중 분류</b>: 각 입력 샘플이 2개 이상의 범주로 구분되는 분류 작업. ex, 손글씨 숫자 분류를 말함 <br>\n",
    "- <b>다중 레이블 분류</b>: 각 입력 샘플이 여러개의 레이블에 할당될 수 있는 분류 작업. ex, 하나의 이미지에 고양이와 강아지가 모두 들어 있을 떄는 '고양이' 레이블과 '강아지' 레이블을 모두 할당해야 한다. 보통 이미지마다 레이블의 개수는 다름 <br>\n",
    "- <b>스칼라 회귀</b>: 타깃이 연속적인 스칼라 값인 작업. 주택 가격 예측이 좋은 예, 각기 다른 타깃 가격이 연속적인 공간을 형성함 <br>\n",
    "- <b>벡터 회귀</b>: 타깃이 연속적인 값의 집합인 작업. 예를 들어 연속적인 값으로 이루어진 벡터. (이미지에 있는 경계 사자 (bounding box)의 좌표 같은) 여러개의 값에 대한 회귀를 한다면 벡터 회귀 <br>\n",
    "- <b>미니 배치</b> 또는 <b>배치</b>: 모델에 의해 동시에 처리되는 소량의 샘플 묶음(일반적으로 8개 에서 128개 사이). 샘플 개수는 GPU 메모리의 할당이 용이하도록 2의 거듭제곱으로 하는 경우가 많다. 훈련할 떄 미니 배치마다 한번씩 모델의 가중치에 적용할 경사 하강법 업데이트 값을 계산 <br><br>\n",
    "\n",
    "\n",
    "### 4.1 영화 리뷰 분류: 이진 분류 문제\n",
    "two-class classification or binary classification이라고도 부름 <br>\n",
    "\n",
    "#### 4.1.1 IMDB 데이터셋 \n",
    "5만개가 있고 부정, 긍정 각각 50%, 50% <br>\n",
    "\n",
    "#### 코드 4-1 IMDB 데이터셋 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b335b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853eff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "218\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(len(train_data[0])) # 218\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b3c464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])  # 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb25cc2",
   "metadata": {},
   "source": [
    "#### 코드 4-2 리뷰를 다시 텍스트로 디코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e83d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])  # 정수 인덱스와 단어를 매핑하도록 뒤집는다\n",
    "decoded_review = \" \".join(\n",
    "    [reverse_word_index.get(i-3, \"?\") for i in train_data[0]]) \n",
    "# 리뷰를 디코딩한다. 0,1,2는 '패딩', '문서 시작', '사전에 없음'을 위해 예약되어 있으므로 인덱스에서 3을 뺌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02890d",
   "metadata": {},
   "source": [
    "### 4.1.2 데이터 준비\n",
    "\n",
    "리스트를 텐서로 바꾸는 2가지 방법)\n",
    "- 리스트에 패딩(padding)을 추가하고 (samples, max_length) 크기의 정수 텐서로 변환 <br>\n",
    "- 리스트를 멀티-핫 인코딩(multi-hot encoding)하여 0과 1의 벡터로 변환 <br>\n",
    "- 여기서는 2번째 방법을 적용\n",
    "\n",
    "#### 코드 4-3 정수 시퀀스를 멀티-핫 인코딩으로 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdabbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def vectorize_sequences(sequences, dimension=100000):\n",
    "    results = np.zeros((len(sequences), dimension))  # 크기가 (len(sequences), dimension)이고 모든 원소가 0인 행렬을 만듦\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i,j] = 1  # results[i]에서 특정 인덱스의 위치를 1로 만듦\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)   # 훈련 데이터를 벡터로 변환함\n",
    "x_test = vectorize_sequences(test_data)   # 테스트 데이터를 벡터로 변홤함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c22705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1861e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "# 신경망에 주입할 데이터가 준비되었음!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e312b",
   "metadata": {},
   "source": [
    "### 4.1.3 신경망 모델 만들기\n",
    "\n",
    "- 레이블은 스칼라(1 또는 0)\n",
    "- Dense층을 쌓을 때 2가지 중요한 구조상의 결정이 필요:\n",
    "    - 얼마나 많은 층을 사용할 것인가? <br>\n",
    "    - 각 층에 얼마나 많은 유닛을 둘 것인가? <br>\n",
    "    (책에서는) <br>\n",
    "    - 16개의 유닛을 가진 2개의 증간층 <br>\n",
    "    - 현재 리뷰의 감정을 scalar 값의 예측으루 출력하는 3번째 층 <br>\n",
    "    \n",
    "#### 코드 4-4 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207ad0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu'), # 첫번째 매개변수는 층의 unit개수\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7e791",
   "metadata": {},
   "source": [
    "output = relu(dot(W, input)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd106f73",
   "metadata": {},
   "source": [
    "#### 활성화 함수? 필요한 이유?\n",
    "\n",
    "- relu와 같은 활성화 함수( or non-linearity)라고도 부름, 이게 없다면 Dense층은 선형적인 연산인 점곱과 덧셈 2개로 구성 <br>\n",
    "output = dot(W, input) + b <br><br>\n",
    "\n",
    "- 선형변환(아핀변환)만 학습 <br>\n",
    "- 이 층의 가설 공간은 입력 데이터를 16차원의 공간으로 바꾸는 가능한 모든 선혀 변환의 집합 <br>\n",
    "- 가설 공간을 풍부하게 만들어 층을 깊게 만드느 장점을 살리기 위해서는 비선형성 or 활성화 함수를 추가해야 함 <br><br>\n",
    "\n",
    "\n",
    "마지막으로 손실 함수와 옵티마이저를 선택 <br>\n",
    "crossentropy는 정보이론(information theory)분야에서 온 개념으로 확률분포간의 차이를 측정 <br>\n",
    "\n",
    "#### 코드 4-5 모델 컴파일하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cad4e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"binary_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4409c7",
   "metadata": {},
   "source": [
    "### 4.1.4 훈련검증\n",
    "\n",
    "검증세트를 활용하여 훈련 과정 중에 모델의 정확도를 모니터링하는 것이 표준 관행.\n",
    "\n",
    "#### 코드 4-6 검증 세트 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8df95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[:10000]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b34ae6",
   "metadata": {},
   "source": [
    "#### 코드 4-7 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "         partial_y_train, \n",
    "         epochs = 10,  # 책에선 20인데, 커널이 자꾸 죽어서 20으로 함\n",
    "         batch_size = 512,\n",
    "         validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc9309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict= history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb832944",
   "metadata": {},
   "source": [
    "#### 코드 4-8 훈련과 검증 손실 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9477c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hitorry_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")  # 'bo'는 파란색점을 의미\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")  # 'b'는 파란색 실선을 의미\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88bda93",
   "metadata": {},
   "source": [
    "#### 코드 4-9 훈련과 검증 정확도 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()  # 그래프를 초기화\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"bo\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 결과가 훈련 손실이 에포크마다 감소하고 훈련 정확도는 에포크마다 증가\n",
    "# 경사 하강법 최적화를 사용했을 떄 반복마다 최소화된느 것이 손실이므로 기대했던 대로\n",
    "# 과대적합(overfitting) 되었다고 할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7a9f4",
   "metadata": {},
   "source": [
    "#### 코드 4-10 모델을 처음부터 다시 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "    \n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss =\"binary_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f0baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ca5fd",
   "metadata": {},
   "source": [
    "### 4.1.5 훈련될 모델로 새로운 데이터에 대해 예측하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c104dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509e5f31",
   "metadata": {},
   "source": [
    "### 4.1.6 추가 실험\n",
    "\n",
    "- 여기서는 최종 분류 층 이전에 2개의 표현층을 사용했음, 1개 or 3개의 표현 층을 사용하고 검증과 테스트 정확도에 어떤 영향을 미치는지 확인 <br>\n",
    "- 층의 유닛을 추가하거나 줄여보세요, 32개의 유닛, 64개의 유닛 <br>\n",
    "- binary_crossentropy대신 mse 손실 함수를 사용 <br>\n",
    "- relu 대신 tanh 활성화 함수(초창기 신경망에서 인기 있었던 함수) <br>\n",
    "\n",
    "### 4.1.7 정리\n",
    "\n",
    "- 원본 데이터를 신경망에 텐서로 주입하기 위해서는 꽤 많은 전처리가 필요\n",
    "- relu활성화 함수와 함께 Dense층을 쌓은 모델은 (감성 분류를 포함하여) 여러 종류의 문제에 적용할 수 있어 앞으로 자주 사용하게 될 것\n",
    "- 이진 분류 문제에서 모델은 하나의 유닛과 sigmoid 활성화 함수를 가진 Dense층으로 끝나야 함. 이 모델의 출력은 확률을 나타내는 0과 1 사이의 스칼라 값\n",
    "- 이진 분류 문제에서 이런 스칼라 시그모이드 출력에 대해 사용할 손실 함수는 binary_crossentropy\n",
    "- rmsprop 옵티마이저는 문제에 상관없이 일바적으로 충분히 좋은 선택\n",
    "- 훈련 데이터에 대해 성능이 향상됨에 따라 신경망은 과대적합되기(overfitting) 시작하고 이전에 본적없는 데이터에서는 결과자 점점 나빠짐, 항상 훈련 세트 이외의 데이터에서 성능을 모니터링해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf552b",
   "metadata": {},
   "source": [
    "## 4.2 뉴스 기사 분류: 다중 분류 문제 \n",
    "\n",
    "- 다중분류(multicalss classification): 클래스가 많음\n",
    "- 단일 레이블 다중 분류(single-label, multiclass classfication)문제: 각 데이터 포인트가 정확히 하나의 범주로 분류됨 <br>\n",
    "- 다중 레이블 다중 분류(multi-label, multiclass classification)문제: 각 데이터 포인트가 여러개의 점주에 속할 수 있다면, 이건 이런 문제 <br>\n",
    "\n",
    "### 4.2.1 로이터 데이터셋\n",
    "\n",
    "#### 코드 4-11 로이터 데이터셋 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02d59f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "2246\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67d253b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5a163e",
   "metadata": {},
   "source": [
    "#### 코드 4-12 로이터 데이터셋을 텍스트로 디코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789413e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index= dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = \" \".join(\n",
    "    [reverse_word_index.get(i-3, \"?\") for i in train_data[0]])  \n",
    "# 0, 1, 2는 '패딩', '문서 시작', '사전에 없음'을 위해 예약되어 있으므로 인덱스에서 3을 뺌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f66ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0])  # 샘플에 연결된 레이블은 토픽의 인덱스로 0과 45 사이의 정수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdabb1b",
   "metadata": {},
   "source": [
    "### 4.2.2 데이터 준비\n",
    "\n",
    "이전의 예제와 동일한 코드를 사용해서 데이터를 벡터로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1bed16",
   "metadata": {},
   "source": [
    "#### 코드 4-13 데이터 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd64cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)  # 훈련 데이터 벡터 변환\n",
    "x_test = vectorize_sequences(test_data)  # 훈련 데이터 벡터 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c20f07",
   "metadata": {},
   "source": [
    "레이블을 벡터로 바꾸는 방법은 2가지: <br>\n",
    "\n",
    "1. 레이블의 리스트를 정수 텐서로 변환하는 것 <br>\n",
    "2. one-hot encoding을 시용 <br>\n",
    "    - one-hot encoding이 범주형 데이터에 널리 사용되기 때문에 범주형 인코딩(categorical encoding)이라고도 부름 <br>\n",
    "    - 이 경우 레이블의 원-핫 인코딩은 각 레이블의 인덱스 자리는 1이고 나머지는 모두 0인 벡터, 밑에 코드는 예시 <br><br>\n",
    "    \n",
    "#### 코드 4-14 레이블 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a97d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] =1.\n",
    "    return results\n",
    "y_train = to_one_hot(train_labels)   # 훈련 레이블 벡터 변환\n",
    "y_test = to_one_hot(test_labels)  # 테스트 레이블 벡터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26db50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(train_labels)\n",
    "y_test = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc94f0",
   "metadata": {},
   "source": [
    "### 4.2.3 모델 구성\n",
    "\n",
    "- 출력 클래스의 개수가 2에서 46개로 늘어남, 출력 공간의차원이 훨씬 커졌다. <br>\n",
    "- 각 층은 잠재적으로 정보의 병목(information bottleneck)이 될 수 있음 <br><br>\n",
    "\n",
    "#### 코드 4-15 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb12e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(46, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321044ca",
   "metadata": {},
   "source": [
    "주목해야할 점 2가지\n",
    "\n",
    "1) 마지막 Dense층의 크기가 46, 각 입력 샘플에 대해 46차원의 벡터를 출력한다는 뜻 <br>\n",
    "2) 마지막 층에 softmax 활성화 함수가 사용, 즉 46차원의 출력 벡터를 만들면 output[i]는 어떤 샘플이 클래스 i에 속할 확률 <br><br>\n",
    "=> 이런 문제에 사용활 최선의 손실 함수는 categorical_crossentropy, 이 함수는 두 확률 분포 사이의 거리를 측정 <br>\n",
    "여기에서는 모델이 출력한 확률 분포와 진짜 레이블의 분포 사이의 거리, 두 분포 사이의 거리를 최소화함으로써 진짜 레이블에 가능한 가까운 출력을 내도록 모델을 훈련하게 된다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9c63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe1575",
   "metadata": {},
   "source": [
    "#### 코드 4-17 검증 세트 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea7731e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/2541949817.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpartial_x_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpartial_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[:10000]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c310f",
   "metadata": {},
   "source": [
    "#### 코드 4-18 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f3cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "         partial_y_train,\n",
    "         epochs=20,  # 20번 줌 \n",
    "         batch_size=512,\n",
    "         validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a57a39",
   "metadata": {},
   "source": [
    "#### 코드 4-19 훈련과 검증 손실 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d35d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(loss)+1)\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe260db",
   "metadata": {},
   "source": [
    "#### 코드 4-20 훈련과 검증 정확도 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2de3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그래프 초기하\n",
    "acc = history.history[\"accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef653b",
   "metadata": {},
   "source": [
    "#### 코드 4-21 모델을 처음부터 다시 시작하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfdb5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation =\"relu\"),\n",
    "    layers.Dense(64, activation =\"relu\"),\n",
    "    layers.Dense(46, activation =\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "model.fit(x_train,\n",
    "         y_train,\n",
    "         epochs=9,\n",
    "         batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "# 이 모델은 805에 까까운 정확도를 달성, 균형 잡힌 이진 분류 문제에서 완전히 무작위로 분류하면 50%의 정확도를 달성\n",
    "# 46개의 클래스가 있고 클래스 비율이 같진 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e84d72",
   "metadata": {},
   "source": [
    "### 4.2.5 새로운 데이터에 대해 예측하기\n",
    "\n",
    "predict 메서드를 호출하면 각 샘플에 대해 46개의 토픽에 대한 클래스 확률 분포를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 항목은 길이가 46인 벡터\n",
    "print(predictions[0].shape) \n",
    "\n",
    "# 이 벡터는 확률 분포를 나타내기 때문에 원소를 모두 더하면 1이 된다.\n",
    "print(np.sum(predictions[0]))\n",
    "\n",
    "# 가장 큰 값이 예측 클래스가 된다. 즉, 가장 확률이 높은 클래스이다.\n",
    "print(np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d011e62b",
   "metadata": {},
   "source": [
    "### 4.2.6 레이블과 손실을 다루는 다른 방법\n",
    "\n",
    "먼저 레이블을 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 텐서로 변환\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35cf82",
   "metadata": {},
   "source": [
    "### 4.2.7 충분히 큰 중간층을 두어야 하는 이유 \n",
    "\n",
    "마지막 출력이 46차원이라면 중간층의 중간 유닛이 46개보다 많이 적어서는 안됨. 46차원보다 훨씬 작은 중간층을 두면 정보의 병목이 어떻게 나타나는지 확인 가능\n",
    "\n",
    "#### 코드 4-22 정보 병목이 있는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(4, activation=\"relu\"),\n",
    "    layers.Dense(46, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "         partial_y_train,\n",
    "         epochs=20,\n",
    "         batch_size=128,\n",
    "         validation_data=(x_val, y_val))\n",
    "\n",
    "# 약 71%로, 8%정도 감소 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4d654",
   "metadata": {},
   "source": [
    "### 4.2.8 추가 실험\n",
    "\n",
    "- 더 크거나 작으 층을 사용해 보세요, 32개의 유닛, 128개의 유닛\n",
    "- 최종 소프트맥스 분류 층 이전에 2개의 중간층을 사용, 1개나 3개의 중간층을 사용하면 달라졌을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744354c",
   "metadata": {},
   "source": [
    "### 4.2.9 정리\n",
    "\n",
    "- N개의 클래스로 데이터 포인트를 분류하려면 모델의 마지막 Dense층의 크기는 N이어야 함 <br>\n",
    "- 단일 레이블, 다중 분류 문제에서는 N개의 클래스에 대한 확률 분포를 출력하기 위해 softmax활성화 함수를 사용 <br>\n",
    "- 이런 문제에서는 범주형 crossentropy를 사용, 이 함수는 모델이 출력한 확률 분포와 타깃 분포 사이의 거리를 최소화한다. <br>\n",
    "- 다중 분류에서 레이블을 다루는 2가지 방법 <br>\n",
    "    - 레이블을 범주형 인코딩(또는 one-hot-encoding)으로 진행하고 categorical_crossentropy손실 함수를 사용 <br>\n",
    "    - 레이블을 정수로 인코딩하고 sparse_categorical_crossnetropy 손실 함수를 사용 <br>\n",
    "- 많은 수의 범주를 분류할때 중간츠으이 크기가 너무 작아 모델에 정보의 병목이 생기지 않도록 해야함 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e90a81",
   "metadata": {},
   "source": [
    "## 4.3 주택 가격 예측: 회귀문제\n",
    "\n",
    "앞의 두 문제는 분류문제라 입력 데이터 포인트의 개별적인 레이블 하나를 예측하는 것이 목적, 또 다른 종류의 머신러닝문제는 개별적인 레이블 대신에 연속적인 값을 예측하는 회귀(Regression)임<br>\n",
    "ex) 기상 데이터가 주어졌을때 내일 기온을 ㅖ측하거나 소프트웨어 명세가 주어졌을 때 소프트웨어 프로젝트가 완료될 시간을 예측\n",
    "\n",
    "cf) 회귀와 logistic regression알고리즘은 다른것, 로지스틱 회귀는 회귀 알고리즘이 아니라 분류 알고리즘임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f618453",
   "metadata": {},
   "source": [
    "### 4.3.1 보스턴 주택 가격 데이터셋\n",
    "\n",
    "- 입력 데이터에 있는 각 특성(feature)의 스케일은 서로 다름\n",
    "\n",
    "#### 코드 4-23 보스턴 주택 데이터셋 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce635d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 0us/step\n",
      "65536/57026 [==================================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = (boston_housing.load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ee12904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a389bc80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.2 42.3 50.  21.1 17.7 18.5 11.3 15.6 15.6 14.4 12.1 17.9 23.1 19.9\n",
      " 15.7  8.8 50.  22.5 24.1 27.5 10.9 30.8 32.9 24.  18.5 13.3 22.9 34.7\n",
      " 16.6 17.5 22.3 16.1 14.9 23.1 34.9 25.  13.9 13.1 20.4 20.  15.2 24.7\n",
      " 22.2 16.7 12.7 15.6 18.4 21.  30.1 15.1 18.7  9.6 31.5 24.8 19.1 22.\n",
      " 14.5 11.  32.  29.4 20.3 24.4 14.6 19.5 14.1 14.3 15.6 10.5  6.3 19.3\n",
      " 19.3 13.4 36.4 17.8 13.5 16.5  8.3 14.3 16.  13.4 28.6 43.5 20.2 22.\n",
      " 23.  20.7 12.5 48.5 14.6 13.4 23.7 50.  21.7 39.8 38.7 22.2 34.9 22.5\n",
      " 31.1 28.7 46.  41.7 21.  26.6 15.  24.4 13.3 21.2 11.7 21.7 19.4 50.\n",
      " 22.8 19.7 24.7 36.2 14.2 18.9 18.3 20.6 24.6 18.2  8.7 44.  10.4 13.2\n",
      " 21.2 37.  30.7 22.9 20.  19.3 31.7 32.  23.1 18.8 10.9 50.  19.6  5.\n",
      " 14.4 19.8 13.8 19.6 23.9 24.5 25.  19.9 17.2 24.6 13.5 26.6 21.4 11.9\n",
      " 22.6 19.6  8.5 23.7 23.1 22.4 20.5 23.6 18.4 35.2 23.1 27.9 20.6 23.7\n",
      " 28.  13.6 27.1 23.6 20.6 18.2 21.7 17.1  8.4 25.3 13.8 22.2 18.4 20.7\n",
      " 31.6 30.5 20.3  8.8 19.2 19.4 23.1 23.  14.8 48.8 22.6 33.4 21.1 13.6\n",
      " 32.2 13.1 23.4 18.9 23.9 11.8 23.3 22.8 19.6 16.7 13.4 22.2 20.4 21.8\n",
      " 26.4 14.9 24.1 23.8 12.3 29.1 21.  19.5 23.3 23.8 17.8 11.5 21.7 19.9\n",
      " 25.  33.4 28.5 21.4 24.3 27.5 33.1 16.2 23.3 48.3 22.9 22.8 13.1 12.7\n",
      " 22.6 15.  15.3 10.5 24.  18.5 21.7 19.5 33.2 23.2  5.  19.1 12.7 22.3\n",
      " 10.2 13.9 16.3 17.  20.1 29.9 17.2 37.3 45.4 17.8 23.2 29.  22.  18.\n",
      " 17.4 34.6 20.1 25.  15.6 24.8 28.2 21.2 21.4 23.8 31.  26.2 17.4 37.9\n",
      " 17.5 20.   8.3 23.9  8.4 13.8  7.2 11.7 17.1 21.6 50.  16.1 20.4 20.6\n",
      " 21.4 20.6 36.5  8.5 24.8 10.8 21.9 17.3 18.9 36.2 14.9 18.2 33.3 21.8\n",
      " 19.7 31.6 24.8 19.4 22.8  7.5 44.8 16.8 18.7 50.  50.  19.5 20.1 50.\n",
      " 17.2 20.8 19.3 41.3 20.4 20.5 13.8 16.5 23.9 20.6 31.5 23.3 16.8 14.\n",
      " 33.8 36.1 12.8 18.3 18.7 19.1 29.  30.1 50.  50.  22.  11.9 37.6 50.\n",
      " 22.7 20.8 23.5 27.9 50.  19.3 23.9 22.6 15.2 21.7 19.2 43.8 20.3 33.2\n",
      " 19.9 22.5 32.7 22.  17.1 19.  15.  16.1 25.1 23.7 28.7 37.2 22.6 16.4\n",
      " 25.  29.8 22.1 17.4 18.1 30.3 17.5 24.7 12.6 26.5 28.7 13.3 10.4 24.4\n",
      " 23.  20.  17.8  7.  11.8 24.4 13.8 19.4 25.2 19.4 19.4 29.1]\n"
     ]
    }
   ],
   "source": [
    "print(train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba98aed",
   "metadata": {},
   "source": [
    "### 4.3.2 데이터 준비\n",
    "\n",
    "상이한 스케일을 가진 값을 신겸ㅇ앙에 주입하면 문제가 됨, 모델이 이런 다양한 데이터에 자동으로 맞추려고 할수 있지만, 꽤 복잡하기 때문에 특성별로 정규화를 진행함\n",
    "\n",
    "#### 코드 4-24 데이터 정규화하기\n",
    "\n",
    "주의할점!\n",
    "- 테스트 데이터를 정규화할 때 사용한 값이 훈련 데이터에서 계산한 값임\n",
    "- 머신러닝 작업 과정에서 절대로 테스트 제이터에서 계산한 어떤 값도 사용해서는 안됨, 지금같은 간단한 데이터 정규화 작업에서도 마찬가지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "test_data = -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bee6a0",
   "metadata": {},
   "source": [
    "### 4.3.3 모델 구성\n",
    "\n",
    "일반적으로 휸련 데이터의 개수가 적을수로 과대적합이 더 쉽게 일어나므로 작은 모델을 사용하는것이 과대적합을 피하는 한 방법\n",
    "\n",
    "#### 코드 4-25 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa4a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([  # 동일한 모델을 여러 번 생성할 것이므로 함수를 만들어 사용\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f594141",
   "metadata": {},
   "source": [
    "- 이 모델의 마지막 층은 하나의 유닛을 가지고 있고 활성화 함수가 없다는 특징, 이를 선형층이라고도 부름. \n",
    "=> 전형적인 scalar regression을 위한 구성\n",
    "\n",
    "- ex) sigmoid 활성화 하무르 적용하면 모델이 0과 1사이의 값을 예측하도록 학습될것임\n",
    "\n",
    "- 이 모델은 mse 손실 함수를 사용하여 컴파일함, Mean squared Error의 약어로 예측과 타깃 사이 거리의 제곱임\n",
    "\n",
    "- 평균 절대 오차(Mean Absolute Error)를 처정함, 예측과 타깃 사이 거리의 절댓값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9099a",
   "metadata": {},
   "source": [
    "### 4.3.4  K-겹 검증을 사용한 훈련 검증\n",
    "\n",
    "- 검증 세트와 훈련 세트로 어떤 데이터 포인트가 선택되었는지에 따라 검증 점수가 크게 달라진다, 즉, 검증 세트의 준할에 대한 검증 점수의 분산이 높음. <br>\n",
    "- 이럴 때 K-ruq교차검증(K-fold cross-validation)을 사용\n",
    "\n",
    "#### 코드 4-26  K-겹 검증하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbabf11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_val_samples = len(train_data)//k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print(f\"#{i}번째 폴드 처리중\")\n",
    "    val_data = train_data[i*num_val_samples:(i+1)*num_val_samples]  # 검증 데이터 준비: k번째 분할\n",
    "    val_targets = train_targets[i*num_val_samples:(i+1)*num_val_samples]  \n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i*num_val_samples],\n",
    "         train_data[(i+1)*num_val_samples:]],\n",
    "        axis=0\n",
    "    )\n",
    "    partial_train_target = np.concatenate(\n",
    "        [train_targets[:i*num_val_samples],\n",
    "         train_targets[(i+1)*num_val_samples:]],\n",
    "        axis=0\n",
    "    )\n",
    "    model = build_model()  # 케라스 모델 구성(컴파일 포함)\n",
    "    model.fit(partial_train_data, partial_train_targetes,  # 모델 훈련(verbose=0)이면 훈련 과정이 출력되지 않ㅇ므\n",
    "             epochs = num_epochs, batch_size=16, verbose=0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)\n",
    "    \n",
    "# num_epochs = 100 이므로 실행하면 다음 결과를 얻는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b48fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 세트가 다르므로 확실히 검증 점수가 2.1에서 3.1까지 변화가 크다, 평균값(2.6)이 각각의 점수보다 훨씬 신뢰할 만함\n",
    "\n",
    "print(all_scores)\n",
    "print(np.mean(all_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad19eb7",
   "metadata": {},
   "source": [
    "#### 코드 4-27 각 폴드의 검증 점수 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print(f\"#{i}번째 폴드 처리중\")\n",
    "    val_data = train_data[i*num_val_samples: (i+1)*num_val_samples]  # 검증 데이터 준비: k번째 분할\n",
    "    val_targets = train_targets[i*num_val_samples: (i+1)*num_val_samples]\n",
    "    partial_train_data = np.concatenate( # 훈련 데이터 준비: 다른 분할 전체\n",
    "        [train_data[:i*num_val_samples],\n",
    "         train_data[(i+1)*num_val_samples:]],\n",
    "        axis=0\n",
    "    )\n",
    "    partial_train_targets = np.concatenate( # 훈련 데이터 준비: 다른 분할 전체\n",
    "        [train_targets[:i*num_val_samples],\n",
    "         train_targets[(i+1)*num_val_samples:]],\n",
    "        axis=0\n",
    "    )\n",
    "    model = build_model() # 케라스 모델 구성(컴파일 포함)\n",
    "    history = model.fit(partial_train_data, partial_train_targets,  # 모델 훈련(verbose=0) 이므로 훈련 과정이 출력되지 않는다.\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                       epochs = num_epochs, batch_size=16, verbose=0)\n",
    "    mae_history = history.history['val_mae']\n",
    "    all_mae_hostories.append(mae_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383f93c",
   "metadata": {},
   "source": [
    "#### 코드 4-28 k-겹 검증 점수 평균 기록하기\n",
    "\n",
    "모든 폴드에 대ㅐㅎ 에포크의 MAE 점수 평균을 계산함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128bb230",
   "metadata": {},
   "source": [
    "#### 코드 4-29 검증 점수 그래프 그리기\n",
    "\n",
    "그래프로 나타내면 그림 4-9와 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()\n",
    "\n",
    "# 이 그래프는 범위가 크기 떄문에 보기가 좀 어렵다, 처음 몇 번의 에포크 동안 MAE가 그 이후 에포크보다 너무 높다. \n",
    "# 나머지 곡선 부북놔 스케일이 크게 다른 처음 10개의 데이터 포인트를 제외시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d8f9a",
   "metadata": {},
   "source": [
    "#### 코드 4-30 처은 10개의 데이터 포인트를 제외한 검증 점수 그래프 그리기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e612efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_mae_history = average_mae_history[10:]\n",
    "plt.plot(range(1, len(truncated_mae_history)+1), truncated_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()\n",
    "\n",
    "# 120번째 ~ 140번째 에포크 이후에 줄어드는 것이 멈춤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e25decd",
   "metadata": {},
   "source": [
    "#### 코드 4-31 최종 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d23da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()  # 새롭게 컴파일된 모델을 얻는다.\n",
    "model.fit(train_data, train_targets, epochs = 30, batch_size = 16, verbose = 0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
    "print(test_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380a436",
   "metadata": {},
   "source": [
    "### 4.3.5 새로운 데이터에 대해 예측하기\n",
    "\n",
    "- 이진 분류 모델에서 predict() 메서드를 호출할 때 각 샘플에 대해 0과 1사이의 scalar 점수가 반영\n",
    "- 이 scalar 회귀 모델의 predict() 메서드를 사용하여 새로운 샘플의 가격을 1000 달러 단위로 예측가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a065ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)\n",
    "print(predictions[0])\n",
    "\n",
    "# 테스트 세트에 있는 첫번째 주택의 가격은 약 1만 달러로 예상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47de9b",
   "metadata": {},
   "source": [
    "### 4.3.6 정리\n",
    "\n",
    "- 회귀는 분류에서 사용했던 것과는 다른 손실함수를 사용, MSE(평균 제곱오차)는 회귀에서 자주 사용되는 손실 함수\n",
    "- 비슷하게 회귀에서 사용되는 평가 지표는 분류와 다름, 정확도 개념은 회귀에 적용되지 않음, 일반적인 회귀 지표는 평균 절대 오차(MAE)이다.\n",
    "- 입력 데이터의 특성이 서로 다른 범위를 가지면 전처리 단계에서 각 특성을 개별적으로 스케일 조정해야 한다\n",
    "- 가용한 데이터가 적다면 K-겹 검증을 사용하는 것이 신뢰할 수 있는 모델 평가 방법\n",
    "- 가용한 훈련 데이터가 적다면 과대적합을 피하기 위해 중간층의 수를 줄인 작은 모델을 사용하는 것이 좋다 (일반적으로 1개 또는 2개)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3214d",
   "metadata": {},
   "source": [
    "## 4.4 요약\n",
    "\n",
    "- 벡터 데이터를 사용하는 가장 일반적인 머신 러닝 작업은 이진분류, 다중 분류, 스칼라 회귀 <br>\n",
    "    - 회귀에서 사용하는 손실 함수와 평가 지표는 분류와 다르다 <br>\n",
    "- 보통 원본 데이터를 신경망에 주입하기 전에 전처리함 <br>\n",
    "- 데이터에 범위가 다른 특성이 있다면 전처리 단계에서 각 특성을 독립적으로 스케일 조정해야함 <br>\n",
    "- 훈련이 진행됨에 따라 신경망의 과대적합이 시작되고 새로운 데이터에 대해 나쁜 결과를 얻게 됨 <br>\n",
    "- 훈렌 데이터가 많지 않으면 과대적합을 피하기 위해 1개 또는 2개의 증간층을 가진 모델을 사용함 <br>\n",
    "- 데이터가 많은 범주로 나뉘어 있을 때 중간층이 너무 작으면 정보의 병목이 생길 수 있음 <br>\n",
    "- 데이터양이 적을 떄는 K-겹 검증이 신뢰할 수 있는 모델 평가를 도와줌 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b0f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6a834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0300aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a687b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee9c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0dec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
