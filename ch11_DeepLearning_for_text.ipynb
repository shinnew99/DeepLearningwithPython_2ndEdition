{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471d0d52",
   "metadata": {},
   "source": [
    "## 11장 텍스트를 위한 딥러닝\n",
    "\n",
    "11.1 자연어 처리 소개 <br>\n",
    "11.2 텍스트 데이터 준비 <br>\n",
    "11.3 단어 그룹을 표현하는 두 가지 방법: 집합과 시퀀스 <br>\n",
    "11.4 텍스트 분류를 넘어: seq-to-seq 학습 <br>\n",
    "11.5 요약 <br>\n",
    "\n",
    "### 11.1 자연어 처리 소개\n",
    "사람의 언어를 assembly, LISP, XML같은 기계를 위해 고안된 언어와 구별하기 위해 그렇게 부름 <br>\n",
    "사람의 언어는 먼저 사용되고 나중에 규칙이 생김, 고정된 어휘에서 정확하게 정의 된 개념을 표현하기 위해 정확한 문법 규칙을 사용함 <br>\n",
    "<b> 언어는 거의 모든 지식을 저장하는 방법, 사람의 생각 자체가 대부분 언어에 기반을 둔다</b> <br>\n",
    "응용 언어학의 입장에서 시도되었음: <br>\n",
    "- 1960s ~ 1990s:  ELIZA 프로그램이 패턴 매칭을 사용해서 매우 기초적인 대화를 수행 <br>\n",
    "- 1980s 후반: 자연어 처리에 머신러닝 방법을 적용, 결정트리 기반-->if/then/else같은 규칙을 자동으로 개발하려는 의도, logistic 회귀와 같은 통계적 방법이 퍼짐 <br>\n",
    "- 1990s: Frederick Jelink은 \"언어학자를 해고할떄마다 음성 인식기 성능이 올라간다\" <br><br>\n",
    "\n",
    "\n",
    "현대적인 NLP는 입력으로 언어를 받아 유용한 어떤 것을 반환하는 것: <br>\n",
    "- \"이 글의 주제는 무엇인가요?\" (텍스트 분류) <br>\n",
    "- \"이 텍스트에 부적절한 내용이 들어있나요?\" (콘텐츠 필터링) <br>\n",
    "- \"이 텍스트가 긍정적인가요? 아니면 부정적인가요?\" (감성 분석) <br>\n",
    "- \"이 문장은 독일어로 어떻게 되나요?\" (번역) <br>\n",
    "- \"이 글을 하나의 문단으로 요약하면 어떻게 되나요?\" (요약) <br>\n",
    "- 기타 <br>\n",
    "\n",
    "모델은 입력에 있는 통계적인 규칙성을 찾는 것뿐이며 이는 여러가지 간단한 작업을 잘 수행하는데 충분합니다. <br>\n",
    "LSTM은 1990s 후반에 나온 시퀀스 처리 알고리즘, 2015년에 Keras가 오픈소스로 제공 <br>\n",
    "2015 ~ 2017 까지 NLP의 급성장, 특히 양방향 LSTM모델이 요약(summarization)에서 질문-대답(question-answering), 기계 번역까지..! <br>\n",
    "2017 ~ 2018 즈음에 Transformer <br><br>\n",
    "\n",
    "\n",
    "### 11.2 텍스트 데이터 준비 \n",
    "<b>Text Vectorization (텍스트 벡터화)</b>는 텍스트를 수치 텐서로 바꾸는 과정, 하는 이유는 미분 가능한 함수인 딥러닝 모델은 수치 텐서만 처리할 수 있다, 원시 텍스트를 입력으로 사용할 수 없다. 다음은 텍스트 벡터화 과정임\n",
    "1. <b>표준화(standardization)</b>한다. 소문자로 바꾸거나 구두점을 제거하는 등의 작업 <br>\n",
    "2. 텍스트를 <b>토큰(Token)</b>단위로 분할한다. 예를 들어 문자, 단어, 단어의 그룹, 이를 <b>Tokenization(토큰화)</b>라고 부른다.<br>\n",
    "3. 각 토큰을 수치 벡터로 바꾼다. 일반적으로 먼저 데이터에 등장하는 모든 토큰을 <b>인덱싱(Indexing)</b>한다.<br><br>\n",
    "\n",
    "\n",
    "#### 11.2.1 텍스트 표준화\n",
    "\"sunset came. i was staring at the Mexico sky. isnt nature splendid??\" <br>\n",
    "\"Sunset came; I stared at the Mexico sky. isn't nature splendid?\" <br>\n",
    "텍스트 표준화는 모델이 인코딩 차이를 고려하지 않도록 이를 제거하기 위한 기초적인 특성 공학의 한 형태, 표준화 방법 중 하나는 '소문자로 바꾸고 구두점 문자를 삭제'하는 것 <br>\n",
    "이렇게 변한다. <br>\n",
    "\"sunset came i was staring at the mexico sky isnt nature splendid\" <br>\n",
    "\"sunset came i stared at the mexico sky isnt nature splendid\" <br><br>\n",
    "\n",
    "또 다른 방법) 머신러닝에서 드물게 사용되는 고급 표준화 패턴 <b>어간 추출(stemming)</b>: <br>\n",
    "(동사의 여러 활용 형태처럼) 어형이 변형된 단어를 공통된 하나의 표현으로 바꾸는 것<br>\n",
    "ex) \"caught\"와 \"been catching\"을 \"[catch]\"로 바꾸거나 \"cats\"를 \"cat\"으로 바꾸는 식 <br>\n",
    "그렇게 되면 비슷한 두 문장이 결국 동일한 인코딩을 가지게 된다. <br><br>\n",
    "\n",
    "\n",
    "#### 11.2.2 텍스트 분할(토큰화)\n",
    "텍스트를 표준화하고 나면 벡터화할 단위(토큰)으로 나누어야 한다. 이 단계를 토크화(Tokenization)이라고 부름. 3가지 방법으로 수행<br>\n",
    "- <b>단어 수준 토큰화</b>: 토큰이 공백으로 (또는 구두점으로) 구분된 부분 문자열 <br>\n",
    "- <b>N-그램(n-gram) 토큰화</b>: 토큰이 N개의 연속된 단어 그룹이다 <br>\n",
    "- <b>문자 수준 토큰화</b>: 각 문자가 하나의 토큰이다. <br>\n",
    "\n",
    "일반적으로 단어 수준 토큰화나 N-그램 토큰화를 항상 사용할것임. 단어싀 순서를 고려하는 시퀀스 모델(sequence model)과 입력 단어의 순서를 무시하고 집합으로 다루는 BoW모델(Bag-of-Words model)이다. \n",
    "\n",
    "__cf) N-그램과 BoW__\n",
    "단어 N-그램은 문장에서 추출한 N개 (또는 그 이하)의 연속된 단어 그룹인데, 2-그램의 집합 또는 3-그램의 집합으로 분해할 수 있음. <br>\n",
    "{the , the cat, cat, cat sat, sat, <br>\n",
    "sat on, on, on the, the mat, mat} <br><br>\n",
    "\n",
    "{the , the cat, cat, cat sat, the cat sat, <br>\n",
    "sat, sat on, on, cat sat on, on the, <br>\n",
    "sat on the, the mat, mat, on the mat} <br><br>\n",
    "\n",
    "이런 집합을 각각 2-그램 가방(bag of 2-gram) 또는 3-그램 가방(bag of 3-gram)이라고 한다. 가방이란 용어는 다루고자 하는 것이 리스트나 시퀀스가 아니라 토큰의 집합이라는 사시을 의미한다. 이 토큰에는 특정한 순서가 없다. <br><br>\n",
    "\n",
    "#### 11.2.3 어휘 사전 인덱싱\n",
    "텍스트를 토큰으로 나눈 후 각 토큰을 수치 표현으로 인코딩해야 한다. 토큰을 해싱(hasing)하여 고정 크기의 이진 벡터로 바꾸는 것처럼 상태가 없는 방식을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c87c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e551dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f02fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d95ab03",
   "metadata": {},
   "source": [
    "#### 코드 11-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fd4c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                 loss=\"binary_rrossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ca286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fb7fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c860904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7183e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeeb100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa22f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c713a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9d093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046f33a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148681fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658c69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c50a26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb33f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933fe211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895a680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5fe64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf0796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c760c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1845c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194866d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebee0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a3490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d84d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a50246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fc808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18e98f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812d3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ab6e783",
   "metadata": {},
   "source": [
    "#### 코드 11-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3c00710",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_998/2943522712.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int64\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TransformerEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"binary_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d52cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3192a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02243d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a71837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bf63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f0c9bd5",
   "metadata": {},
   "source": [
    "https://docs.google.com/presentation/d/1kCPzd2BDLwKkTGXD9___vAKtZl5iYslXub1bp7dDhzg/edit?pli=1#slide=id.p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
