{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35abd9ab",
   "metadata": {},
   "source": [
    "## 5 머신러닝의 기본요소\n",
    "\n",
    "1 일반화:머신 러닝의 목표 <br>\n",
    "2 머신 러닝 모델 평가 <br>\n",
    "3 훈련 성능 향상하기 <br>\n",
    "4 일반화 성능 향상하기\n",
    "5 요약 <br><br>\n",
    "\n",
    "여기서는 정확한 모델평가의 중요성 및 훈련과 일반화 사이의 균형을 강조하면서 머신러닝에 대한 새로우 직관을 확고한 개념으로 정립 <br>\n",
    "\n",
    "\n",
    "### 5.1 일반화: 머신러닝의 목표\n",
    "\n",
    "머신러닝의 근본적인 이슈는 최적화와 일반화 사이의 줄다리기 <br>, \n",
    "- <b>최적화</b>는 가능한 훈련 데이터에서 최긔어 성능을 얻으려고 모델을 조정하는 과정(머신러닝에서 학습에 해당됨) <br>\n",
    "- <b>일반화</b>는 훈련된 모델이 이전에 본적 없는 데이터에서 얼마나 잘 수행되는지 의미, 목표는 좋은 일반화 성능을 얻는 것, 일반화 성능을 제어할 방법은 없음. 단지 모델을 훈련 데이터에 맞출 수만 있음. <br>\n",
    "\n",
    "#### 5.1.1 과소적합과 과대적합\n",
    "- <b>과소적합</b>은 훈련 데이터의 손실이 낮아질수록 테스트 데이터의 손실도 낮아짐, 즉, 네트워크가 훈련 데이터에 있는 모든 관련 패턴을 학습하지 못했음 <br>\n",
    "- But, 훈련 데이터에서 훈련을 특정 횟수만큼 반복하고 난 후에는 일반화 성능이 더 이상 높아지지 않으며 검증 세트의 성능이 멈추고 감소되기 시작함 <br>\n",
    "\n",
    "- <b>잡은 섞인 훈련 데이터</b> <br>: 모델을 이상치에 맞추려고 하면 일반화 성능이 감소됨\n",
    "- <b>불확실한 특성</b> <br>:\n",
    "- <b>드문 특성과 가짜 상관관계</b> <br>: 가짜 상관관계를 만들어 내는 데 특성 값이 몇번만 등장할 필요가 없다는 것.\n",
    "\n",
    "#### 5-1 MNIST에 백색 잡음 픽셀과 0픽셀 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5511cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype(\"float32\")/255\n",
    "train_images_with_noise_channels = np.concatenate([train_images, np.random.random((len(train_images), 784))], axis=1)\n",
    "train_images_with_zeros_channels = np.concatenate([train_images, np.zeros((len(train_images), 784))], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e2512",
   "metadata": {},
   "source": [
    "#### 코드 5-2 백색 잡음과 0을 추가한 MNIST 데이터에서 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15abe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 13s 32ms/step - loss: 0.6244 - accuracy: 0.8087 - val_loss: 0.3080 - val_accuracy: 0.9045\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 12s 31ms/step - loss: 0.2589 - accuracy: 0.9197 - val_loss: 0.2052 - val_accuracy: 0.9409\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 12s 31ms/step - loss: 0.1658 - accuracy: 0.9490 - val_loss: 0.1649 - val_accuracy: 0.9517\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 12s 31ms/step - loss: 0.1167 - accuracy: 0.9641 - val_loss: 0.1738 - val_accuracy: 0.9502\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 12s 31ms/step - loss: 0.0887 - accuracy: 0.9729 - val_loss: 0.1304 - val_accuracy: 0.9632\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 12s 32ms/step - loss: 0.0655 - accuracy: 0.9785 - val_loss: 0.1751 - val_accuracy: 0.9529\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 12s 31ms/step - loss: 0.0483 - accuracy: 0.9845 - val_loss: 0.1177 - val_accuracy: 0.9683\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 12s 31ms/step - loss: 0.0355 - accuracy: 0.9886 - val_loss: 0.1213 - val_accuracy: 0.9687\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 12s 32ms/step - loss: 0.0277 - accuracy: 0.9912 - val_loss: 0.1278 - val_accuracy: 0.9689\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 12s 31ms/step - loss: 0.0227 - accuracy: 0.9927 - val_loss: 0.1323 - val_accuracy: 0.9695\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(512, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                 loss=\"sparse_categorical_crossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "history_noise = model.fit(\n",
    "    train_images_with_noise_channels, train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941e6aa8",
   "metadata": {},
   "source": [
    "#### 코드 5-3 검증 정확도 비교 그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce982d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_acc_noise = history_noise.history[\"val_accuracy\"]\n",
    "val_acc_zeros = history_noise.history[\"val_accuracy\"]\n",
    "epochs = range(1, 11)\n",
    "plt.plot(epochs, val_acc_noise, \"b--\",\n",
    "        label = \"Validation accuracy with noise channels\")\n",
    "plt.plot(epochs, val_acc_zeros, \"b--\",\n",
    "        label = \"Validation accuracy with zeros channels\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 잡음을 더 많이 섞을수록 정확도는 더 감소, 잡음 특성은 필연적으로 과대적합을 유발시킴\n",
    "# 특성 선택을 수행하는 것이 일반적, 특선 선택을 하는 일반적인 방법은 가용한 각 특성에 대해 어떤 유용성 점수를 계산하는 것\n",
    "# 즉, 특성과 레이블 사이의 상호 의존 정보(mutual information)처럼 작업에 대해 특성이 얼마나 유익한지 측정\n",
    "# 그다음 임계값을 넘긴 특성만 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b5228d",
   "metadata": {},
   "source": [
    "#### 코드 5-4 랜덤하게 섞은 레이블로 MNIST 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1336a7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 2.3866 - accuracy: 0.1018 - val_loss: 2.3258 - val_accuracy: 0.0998\n",
      "Epoch 2/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 2.3090 - accuracy: 0.1118 - val_loss: 2.3154 - val_accuracy: 0.1061\n",
      "Epoch 3/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 2.2961 - accuracy: 0.1244 - val_loss: 2.3227 - val_accuracy: 0.0984\n",
      "Epoch 4/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 2.2842 - accuracy: 0.1353 - val_loss: 2.3241 - val_accuracy: 0.1013\n",
      "Epoch 5/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 2.2684 - accuracy: 0.1485 - val_loss: 2.3319 - val_accuracy: 0.1016\n",
      "Epoch 6/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 2.2495 - accuracy: 0.1623 - val_loss: 2.3479 - val_accuracy: 0.1022\n",
      "Epoch 7/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 2.2267 - accuracy: 0.1785 - val_loss: 2.3616 - val_accuracy: 0.0953\n",
      "Epoch 8/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 2.1996 - accuracy: 0.1939 - val_loss: 2.3699 - val_accuracy: 0.1022\n",
      "Epoch 9/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 2.1706 - accuracy: 0.2076 - val_loss: 2.3925 - val_accuracy: 0.1015\n",
      "Epoch 10/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 2.1398 - accuracy: 0.2271 - val_loss: 2.4160 - val_accuracy: 0.1010\n",
      "Epoch 11/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 2.1072 - accuracy: 0.2402 - val_loss: 2.4514 - val_accuracy: 0.0994\n",
      "Epoch 12/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 2.0721 - accuracy: 0.2581 - val_loss: 2.4724 - val_accuracy: 0.1041\n",
      "Epoch 13/100\n",
      "375/375 [==============================] - 8s 22ms/step - loss: 2.0365 - accuracy: 0.2741 - val_loss: 2.4978 - val_accuracy: 0.1013\n",
      "Epoch 14/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 2.0015 - accuracy: 0.2887 - val_loss: 2.5330 - val_accuracy: 0.1049\n",
      "Epoch 15/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.9672 - accuracy: 0.3037 - val_loss: 2.5576 - val_accuracy: 0.1053\n",
      "Epoch 16/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 1.9324 - accuracy: 0.3174 - val_loss: 2.5804 - val_accuracy: 0.0972\n",
      "Epoch 17/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.8958 - accuracy: 0.3312 - val_loss: 2.6194 - val_accuracy: 0.0965\n",
      "Epoch 18/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.8635 - accuracy: 0.3451 - val_loss: 2.6409 - val_accuracy: 0.0994\n",
      "Epoch 19/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 1.8277 - accuracy: 0.3602 - val_loss: 2.6866 - val_accuracy: 0.1036\n",
      "Epoch 20/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.7945 - accuracy: 0.3735 - val_loss: 2.7159 - val_accuracy: 0.0986\n",
      "Epoch 21/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 1.7604 - accuracy: 0.3885 - val_loss: 2.7698 - val_accuracy: 0.1032\n",
      "Epoch 22/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.7261 - accuracy: 0.4007 - val_loss: 2.7934 - val_accuracy: 0.0995\n",
      "Epoch 23/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 1.6951 - accuracy: 0.4117 - val_loss: 2.8289 - val_accuracy: 0.1002\n",
      "Epoch 24/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 1.6629 - accuracy: 0.4246 - val_loss: 2.8865 - val_accuracy: 0.0987\n",
      "Epoch 25/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 1.6320 - accuracy: 0.4345 - val_loss: 2.9334 - val_accuracy: 0.1010\n",
      "Epoch 26/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 1.6018 - accuracy: 0.4473 - val_loss: 2.9764 - val_accuracy: 0.0962\n",
      "Epoch 27/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 1.5719 - accuracy: 0.4618 - val_loss: 3.0044 - val_accuracy: 0.0995\n",
      "Epoch 28/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 1.5433 - accuracy: 0.4712 - val_loss: 3.0515 - val_accuracy: 0.0982\n",
      "Epoch 29/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 1.5148 - accuracy: 0.4827 - val_loss: 3.1042 - val_accuracy: 0.1008\n",
      "Epoch 30/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.4883 - accuracy: 0.4908 - val_loss: 3.1608 - val_accuracy: 0.0994\n",
      "Epoch 31/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.4592 - accuracy: 0.5019 - val_loss: 3.1979 - val_accuracy: 0.1005\n",
      "Epoch 32/100\n",
      "375/375 [==============================] - 8s 22ms/step - loss: 1.4338 - accuracy: 0.5096 - val_loss: 3.2526 - val_accuracy: 0.0988\n",
      "Epoch 33/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 1.4075 - accuracy: 0.5191 - val_loss: 3.2825 - val_accuracy: 0.0996\n",
      "Epoch 34/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.3819 - accuracy: 0.5304 - val_loss: 3.3257 - val_accuracy: 0.0965\n",
      "Epoch 35/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.3578 - accuracy: 0.5384 - val_loss: 3.3617 - val_accuracy: 0.0983\n",
      "Epoch 36/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 1.3339 - accuracy: 0.5495 - val_loss: 3.4327 - val_accuracy: 0.0979\n",
      "Epoch 37/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.3109 - accuracy: 0.5561 - val_loss: 3.4771 - val_accuracy: 0.1012\n",
      "Epoch 38/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.2874 - accuracy: 0.5655 - val_loss: 3.5161 - val_accuracy: 0.1053\n",
      "Epoch 39/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.2651 - accuracy: 0.5708 - val_loss: 3.5821 - val_accuracy: 0.1028\n",
      "Epoch 40/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.2435 - accuracy: 0.5796 - val_loss: 3.6117 - val_accuracy: 0.0988\n",
      "Epoch 41/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.2210 - accuracy: 0.5883 - val_loss: 3.6723 - val_accuracy: 0.1028\n",
      "Epoch 42/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.2009 - accuracy: 0.5938 - val_loss: 3.7543 - val_accuracy: 0.1011\n",
      "Epoch 43/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.1791 - accuracy: 0.6046 - val_loss: 3.7597 - val_accuracy: 0.1030\n",
      "Epoch 44/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.1580 - accuracy: 0.6100 - val_loss: 3.8399 - val_accuracy: 0.1018\n",
      "Epoch 45/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.1399 - accuracy: 0.6180 - val_loss: 3.8835 - val_accuracy: 0.0968\n",
      "Epoch 46/100\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 1.1213 - accuracy: 0.6272 - val_loss: 3.9367 - val_accuracy: 0.1013\n",
      "Epoch 47/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.1001 - accuracy: 0.6333 - val_loss: 4.0077 - val_accuracy: 0.0979\n",
      "Epoch 48/100\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 1.0817 - accuracy: 0.6359 - val_loss: 4.0418 - val_accuracy: 0.0997\n",
      "Epoch 49/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.0646 - accuracy: 0.6447 - val_loss: 4.1097 - val_accuracy: 0.0984\n",
      "Epoch 50/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.0469 - accuracy: 0.6510 - val_loss: 4.1856 - val_accuracy: 0.1005\n",
      "Epoch 51/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.0274 - accuracy: 0.6576 - val_loss: 4.2166 - val_accuracy: 0.0961\n",
      "Epoch 52/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 1.0103 - accuracy: 0.6626 - val_loss: 4.2615 - val_accuracy: 0.0989\n",
      "Epoch 53/100\n",
      "375/375 [==============================] - 8s 22ms/step - loss: 0.9937 - accuracy: 0.6690 - val_loss: 4.3129 - val_accuracy: 0.0983\n",
      "Epoch 54/100\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 0.9772 - accuracy: 0.6760 - val_loss: 4.3691 - val_accuracy: 0.1008\n",
      "Epoch 55/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.9619 - accuracy: 0.6801 - val_loss: 4.4699 - val_accuracy: 0.0971\n",
      "Epoch 56/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.9444 - accuracy: 0.6871 - val_loss: 4.4733 - val_accuracy: 0.1008\n",
      "Epoch 57/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.9281 - accuracy: 0.6924 - val_loss: 4.5773 - val_accuracy: 0.0997\n",
      "Epoch 58/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.9133 - accuracy: 0.6980 - val_loss: 4.6081 - val_accuracy: 0.1020\n",
      "Epoch 59/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.8974 - accuracy: 0.7027 - val_loss: 4.6462 - val_accuracy: 0.0980\n",
      "Epoch 60/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.8841 - accuracy: 0.7088 - val_loss: 4.7049 - val_accuracy: 0.1006\n",
      "Epoch 61/100\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 0.8701 - accuracy: 0.7150 - val_loss: 4.7959 - val_accuracy: 0.1014\n",
      "Epoch 62/100\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 0.8551 - accuracy: 0.7170 - val_loss: 4.8681 - val_accuracy: 0.1034\n",
      "Epoch 63/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.8422 - accuracy: 0.7200 - val_loss: 4.9220 - val_accuracy: 0.0987\n",
      "Epoch 64/100\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 0.8272 - accuracy: 0.7290 - val_loss: 4.9873 - val_accuracy: 0.1014\n",
      "Epoch 65/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.8132 - accuracy: 0.7334 - val_loss: 5.0179 - val_accuracy: 0.0999\n",
      "Epoch 66/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.7995 - accuracy: 0.7368 - val_loss: 5.0810 - val_accuracy: 0.1009\n",
      "Epoch 67/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.7857 - accuracy: 0.7427 - val_loss: 5.1954 - val_accuracy: 0.1020\n",
      "Epoch 68/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.7723 - accuracy: 0.7473 - val_loss: 5.2566 - val_accuracy: 0.0978\n",
      "Epoch 69/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.7612 - accuracy: 0.7516 - val_loss: 5.3045 - val_accuracy: 0.1014\n",
      "Epoch 70/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.7475 - accuracy: 0.7552 - val_loss: 5.3457 - val_accuracy: 0.1004\n",
      "Epoch 71/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.7377 - accuracy: 0.7584 - val_loss: 5.4430 - val_accuracy: 0.1013\n",
      "Epoch 72/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.7266 - accuracy: 0.7629 - val_loss: 5.4535 - val_accuracy: 0.1021\n",
      "Epoch 73/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.7132 - accuracy: 0.7662 - val_loss: 5.5493 - val_accuracy: 0.0975\n",
      "Epoch 74/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.7017 - accuracy: 0.7706 - val_loss: 5.5827 - val_accuracy: 0.1022\n",
      "Epoch 75/100\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 0.6913 - accuracy: 0.7771 - val_loss: 5.6711 - val_accuracy: 0.1013\n",
      "Epoch 76/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.6780 - accuracy: 0.7789 - val_loss: 5.7575 - val_accuracy: 0.0997\n",
      "Epoch 77/100\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 0.6693 - accuracy: 0.7811 - val_loss: 5.8103 - val_accuracy: 0.1013\n",
      "Epoch 78/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.6554 - accuracy: 0.7886 - val_loss: 5.8540 - val_accuracy: 0.0999\n",
      "Epoch 79/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.6473 - accuracy: 0.7873 - val_loss: 5.9162 - val_accuracy: 0.1010\n",
      "Epoch 80/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.6345 - accuracy: 0.7938 - val_loss: 6.0165 - val_accuracy: 0.1017\n",
      "Epoch 81/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.6265 - accuracy: 0.7978 - val_loss: 6.0534 - val_accuracy: 0.1004\n",
      "Epoch 82/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.6159 - accuracy: 0.8004 - val_loss: 6.1814 - val_accuracy: 0.1031\n",
      "Epoch 83/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.6085 - accuracy: 0.8025 - val_loss: 6.2512 - val_accuracy: 0.0987\n",
      "Epoch 84/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.5951 - accuracy: 0.8071 - val_loss: 6.2914 - val_accuracy: 0.1001\n",
      "Epoch 85/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.5875 - accuracy: 0.8103 - val_loss: 6.3408 - val_accuracy: 0.1031\n",
      "Epoch 86/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.5792 - accuracy: 0.8122 - val_loss: 6.4012 - val_accuracy: 0.0987\n",
      "Epoch 87/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.5697 - accuracy: 0.8147 - val_loss: 6.4928 - val_accuracy: 0.1007\n",
      "Epoch 88/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.5605 - accuracy: 0.8200 - val_loss: 6.5391 - val_accuracy: 0.1013\n",
      "Epoch 89/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.5502 - accuracy: 0.8227 - val_loss: 6.6248 - val_accuracy: 0.0994\n",
      "Epoch 90/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.5411 - accuracy: 0.8254 - val_loss: 6.6709 - val_accuracy: 0.1025\n",
      "Epoch 91/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.5314 - accuracy: 0.8269 - val_loss: 6.7568 - val_accuracy: 0.1008\n",
      "Epoch 92/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.5251 - accuracy: 0.8324 - val_loss: 6.8509 - val_accuracy: 0.1013\n",
      "Epoch 93/100\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 0.5153 - accuracy: 0.8357 - val_loss: 6.9109 - val_accuracy: 0.0959\n",
      "Epoch 94/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.5100 - accuracy: 0.8378 - val_loss: 7.0117 - val_accuracy: 0.0962\n",
      "Epoch 95/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.4998 - accuracy: 0.8391 - val_loss: 7.0376 - val_accuracy: 0.0979\n",
      "Epoch 96/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.4927 - accuracy: 0.8417 - val_loss: 7.0665 - val_accuracy: 0.0996\n",
      "Epoch 97/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.4845 - accuracy: 0.8466 - val_loss: 7.1385 - val_accuracy: 0.0957\n",
      "Epoch 98/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.4763 - accuracy: 0.8471 - val_loss: 7.2250 - val_accuracy: 0.1004\n",
      "Epoch 99/100\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.4672 - accuracy: 0.8516 - val_loss: 7.3178 - val_accuracy: 0.0997\n",
      "Epoch 100/100\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.4628 - accuracy: 0.8530 - val_loss: 7.3954 - val_accuracy: 0.0972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2249973e20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype(\"float32\")/255\n",
    "\n",
    "random_train_labels = train_labels[:]\n",
    "np.random.shuffle(random_train_labels)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "model.fit(train_images, random_train_labels,\n",
    "         epochs=100,\n",
    "         batch_size=128,\n",
    "         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219742e",
   "metadata": {},
   "source": [
    "<b>매니폴드 가설</b>은 실제 세상의 모든 데이터가 (이 데이터가 인코딩된) 고차원 공간안에 있는 저차원 매니폴드에 놓여있다고 가정 <br><br>\n",
    "- 머신러닝 모델은 가능한 입력 공간안에서 비교적 간단하고, 저차원이며, 매우 조저직은 부분공간만 학습하면 된다.<br>\n",
    "- 하나 안에서 두 입력 사이를 보간(interpolation)하는 것이 항상 가능함, 즉, 연속적인 경로를 따라 한 입력에서 다른 입력으로 변형할 때 모든 모인트가 이 매니폴드에 속함 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d16f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f45c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3625026b",
   "metadata": {},
   "source": [
    "#### 코드 5-5 홀드아웃 검증 구현 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa375578",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = 10000\n",
    "np.random.shuffle(data)  # 일반적으로 데이터를 섞는것이 좋다\n",
    "validation_data = data[:num_validation_samples]  # 검증 세트를 만든다.\n",
    "training_data = data[num_validation_samples:]  # 훈련 세트를 만든다.\n",
    "model = get_model()\n",
    "model.fit(training_data, ...)\n",
    "vlidation_score = model.evaluate(validation_data, ...)\n",
    "model = get_model()\n",
    "model.fit(np.concatenate([training_data, validation_data]), ...)\n",
    "test_score = model.evaluate(test_data, ...)\n",
    "\n",
    "# 단점: 데이터가 적ㅇ르떄는 검증 세트와 테스트 세트의 샘플이 너무 적어 주어진 전체 데이터를 통계적으로 대표하지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d31a0",
   "metadata": {},
   "source": [
    "#### K-fold 교차 검증\n",
    "\n",
    "동일한 크기를 가진 K개의 분할로 나눔 <br>\n",
    "↓ <br>\n",
    "각 분할 i에 대해 남은 K-1개의 분할로 모델을 훈련하고 분할 i에서 모델을 평가 <br> \n",
    "↓ <br>\n",
    "최종 점수는 이렇게 얻은 K개의 점수를 평균 <br><br>\n",
    "\n",
    "\n",
    "#### 코드 5-6 K-겹 교차 검증 구현 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "num_validation_samples = len(data)//k\n",
    "np.random.shuffle(data)\n",
    "validation_scores =[]\n",
    "for fold in range(k):\n",
    "    validation_data = data[num_validation_samples*fold:\n",
    "                        num_validation_samples*(fold+1)]\n",
    "    training_data = np.concatenate(\n",
    "                    data[:num_validation_sampels*fold],\n",
    "                    data[:num_validation_sampels*(fold+1):])\n",
    "    model = get_model() # 훈련되지 않은 새로운 모델을 만듦\n",
    "    model.fit(training_data, ...)\n",
    "    validation_score = model.evaluate(validation_data, ...)\n",
    "    validation_scores.append(validation_data)\n",
    "validation_score = np.average(validation_scores)  # 검증점수: K개의 폴드 검증 점수 평균\n",
    "model = get_model() # 테스트 데이터를 제외한 전체 데이터로 최종 모델을 훈련\n",
    "model.fit(data, ...)\n",
    "test_score = model.evaluate(etst_data, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf81874",
   "metadata": {},
   "source": [
    "#### 셔플링을 사용한 반복 K-겹 교차 검증\n",
    "\n",
    "- 비교적 가용 데이터가 적고 가능한 정확하게 모델을 평가하고자 할떄 사용 <br>\n",
    "- K-겹 교차 검증을 여러 번 적용하되 K개의 분할로 나누기 전에 매전 데이터를 무작위로 섞음 <br>\n",
    "- 최종 점수는 모든 K-겹 교차 검증을 실행해서 얻은 점수의 평균 <br>\n",
    "- 결국 P*K개(P는 반복횟수)의 모델을 훈련하고 평가하므로 비용이 매우 많이 든다 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e0669",
   "metadata": {},
   "source": [
    "### 5.2.2 상식 수준의 기준점 넘기\n",
    "\n",
    "항상 넘어야 할 간단한 기준점을 정해야 한다. \n",
    "\n",
    "\n",
    "### 5.2.3 모델 평가에 대해 유념해야할 점\n",
    "- 대표성 있는 데이터: 훈련세트에는 0~7 숫자만 담겨 있고 테스트 세트에는 8~9 숫자만 담기게 될 수도 있기 떄문에 훈련세트와 테스트 세트로 나누기 전에 데이터를 무작위로 섞는것이 일반적이다 <br>\n",
    "- 시간의 방향: 무작위로 섞어서는 절대 안된다, 미래의 정보가 누설되기 때문에. 이런 문제에서는 훈련 세트에 있는 데이터보다 테스트 세트에 있는 모든 데이터가 미래의 것이어야 한다. <br>\n",
    "- 데이터 중복: 절대 안됨 <br>\n",
    "\n",
    "## 5.3 훈련 성능 향상하기\n",
    "\n",
    "최적적합 모델을 얻으려면 먼저 과대적합되어야 한다, 이 경계가 어디인지 미리 알지 못하기 때문에 경계를 찾으려면 넘어가 봐야한다, => 이 문제를 다루기 시작할 때 초기 목표는 약간의 일반화 능력을 보이고 과대적합할 수 있는 모델을 얻는 것이다. <br>\n",
    "이 때 발생할수 있는 문제 3가지: <br>\n",
    "1. 훈련이 되지 않는다: 시간이 지나도 훈련 손실이 줄어들지 않는다. <br>\n",
    "2. 훈련은 잘 시작되었지만 모델이 의미 있는 일반화를 달성하지 못한다: 상식 수준의 기준점을 넘어설 수 없다. <br>\n",
    "3. 시간이 지남에 따라 훈련과 검증 손실이 모두 줄어들고 기준점을 넘어설 수 있지만 과데적합되지 않을 것 같다. 여전히 과소적합 상태 <br>\n",
    "\n",
    "=> 이를 해결하기 위해 1번째 큰 이정표(상식 수준의 기준점을 넘을 수 있어 약간의 일반화 능력이 있고 과대적합할 수 있는 모델을 얻는 것)를 달성하는 방법을 알아겠음 <br><br>\n",
    "\n",
    "### 5.3.1 경사하강법의 핵심 파라미터 튜닝하기\n",
    "\n",
    "훈련이 시작되지 않거나 너무 일찍 중단됨, 손실도 멈춰 있음 => 랜덤한 데이터에서도 모델을 훈련할 수 있다! <br>\n",
    "\n",
    "- 옵티마이저 선택, 모델 가중치의 초깃값 분포, 학습률 ,배치크기 => 이 모든 파라미터는 상호 의존적이다. <br><br>\n",
    "\n",
    "\n",
    "#### 코드 5-7 잘못된 높은 학습률로 MNIST 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ed4e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 323.6319 - accuracy: 0.2905 - val_loss: 2.3413 - val_accuracy: 0.2395\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 3.6882 - accuracy: 0.2085 - val_loss: 2.2544 - val_accuracy: 0.2160\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 4.1423 - accuracy: 0.2157 - val_loss: 2.6029 - val_accuracy: 0.1931\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 2.8359 - accuracy: 0.2138 - val_loss: 2.3223 - val_accuracy: 0.2255\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 2.4354 - accuracy: 0.2303 - val_loss: 10.7874 - val_accuracy: 0.2418\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 2.7866 - accuracy: 0.2279 - val_loss: 2.0127 - val_accuracy: 0.2438\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 2.4076 - accuracy: 0.2338 - val_loss: 2.4589 - val_accuracy: 0.2039\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 2.5314 - accuracy: 0.2353 - val_loss: 2.0699 - val_accuracy: 0.2331\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 2.5195 - accuracy: 0.2302 - val_loss: 2.1112 - val_accuracy: 0.2414\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 2.5251 - accuracy: 0.2431 - val_loss: 4.6280 - val_accuracy: 0.2503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f22498528b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype(\"float32\")/255\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(512, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1.),\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split =0.2)\n",
    "\n",
    "# 이 모델은 30~40%이고 이를 넘어서지 못한다, lr을 1e-2로 낮춰보겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae290c",
   "metadata": {},
   "source": [
    "#### 코드 5-8 같은 모델을 적절한 학습률로 훈련하기\n",
    "\n",
    "이제 모델 훈련이 가능하다, 만약 또 발생하면 다음을 시도해볼수 있다. <br>\n",
    "- 학습률을 낮추거나 높인다 <br>\n",
    "- 배치 크기를 증가시킨다 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fce5bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.3620 - accuracy: 0.9109 - val_loss: 0.1662 - val_accuracy: 0.9554\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.1408 - accuracy: 0.9639 - val_loss: 0.1759 - val_accuracy: 0.9626\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.1130 - accuracy: 0.9731 - val_loss: 0.2335 - val_accuracy: 0.9534\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 7s 20ms/step - loss: 0.0974 - accuracy: 0.9784 - val_loss: 0.2018 - val_accuracy: 0.9688\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.0896 - accuracy: 0.9810 - val_loss: 0.2096 - val_accuracy: 0.9703\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 8s 21ms/step - loss: 0.0783 - accuracy: 0.9845 - val_loss: 0.2329 - val_accuracy: 0.9725\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.0721 - accuracy: 0.9862 - val_loss: 0.2424 - val_accuracy: 0.9709\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.0651 - accuracy: 0.9876 - val_loss: 0.2838 - val_accuracy: 0.9700\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.0595 - accuracy: 0.9894 - val_loss: 0.3054 - val_accuracy: 0.9713\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 8s 20ms/step - loss: 0.0547 - accuracy: 0.9901 - val_loss: 0.3363 - val_accuracy: 0.9707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2249767610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-2),\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed895c",
   "metadata": {},
   "source": [
    "### 5.3.2 구조에 대해 더 나은 가정하기 \n",
    "\n",
    "- 모델이 훈련되지만 어떤 이유에서인지 검증 지표가 전혀 나아지지 않는다. 즉, 모델이 훈련되지만 일반화되지 않는다. <br>\n",
    "최악의 머신러닝 상황: 몇가지 팁 <br>\n",
    "    1. 단순하게 입력 데이터에 타깃 예측을 위한 정보가 충분하지 않을 수 있음 <br>\n",
    "    2. 현재 사용하는 모델의 종류가 문제에 적합하지 않을 수 있음 <br>\n",
    "        - 위의 모델은 순환신경망이 더 적합, 일반화도 더 잘된다. <br>\n",
    "            즉, 구조에 대한 올바른 가정을 내려야 한다. <br>\n",
    "\n",
    "\n",
    "### 5.3.3 모델 용량 늘리기\n",
    "\n",
    "#### 코드 5-9 MNIST 데이터를 사용한 간단한 로지스틱 회귀 모델\n",
    "\n",
    "작은 모델, 간단한 logistic regression을 살펴보면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61540d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.6555 - accuracy: 0.8366 - val_loss: 0.3563 - val_accuracy: 0.9046\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3504 - accuracy: 0.9027 - val_loss: 0.3076 - val_accuracy: 0.9148\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3152 - accuracy: 0.9123 - val_loss: 0.2915 - val_accuracy: 0.9197\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2989 - accuracy: 0.9163 - val_loss: 0.2833 - val_accuracy: 0.9212\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2896 - accuracy: 0.9195 - val_loss: 0.2750 - val_accuracy: 0.9243\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2829 - accuracy: 0.9210 - val_loss: 0.2735 - val_accuracy: 0.9235\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2778 - accuracy: 0.9225 - val_loss: 0.2696 - val_accuracy: 0.9265\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2742 - accuracy: 0.9238 - val_loss: 0.2673 - val_accuracy: 0.9256\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2712 - accuracy: 0.9245 - val_loss: 0.2656 - val_accuracy: 0.9276\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2687 - accuracy: 0.9256 - val_loss: 0.2636 - val_accuracy: 0.9275\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2667 - accuracy: 0.9260 - val_loss: 0.2634 - val_accuracy: 0.9284\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2644 - accuracy: 0.9272 - val_loss: 0.2629 - val_accuracy: 0.9294\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2628 - accuracy: 0.9277 - val_loss: 0.2637 - val_accuracy: 0.9289\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2616 - accuracy: 0.9283 - val_loss: 0.2617 - val_accuracy: 0.9308\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2606 - accuracy: 0.9286 - val_loss: 0.2625 - val_accuracy: 0.9297\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2592 - accuracy: 0.9295 - val_loss: 0.2624 - val_accuracy: 0.9294\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2580 - accuracy: 0.9295 - val_loss: 0.2616 - val_accuracy: 0.9303\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2572 - accuracy: 0.9301 - val_loss: 0.2602 - val_accuracy: 0.9302\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2561 - accuracy: 0.9299 - val_loss: 0.2617 - val_accuracy: 0.9304\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2554 - accuracy: 0.9306 - val_loss: 0.2624 - val_accuracy: 0.9302\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([layers.Dense(10, activation=\"softmax\")])\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "history_small_model = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=20, batch_size=128,\n",
    "    validation_split = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fccd0c",
   "metadata": {},
   "source": [
    "다음과 같은 손실 곡선을 얻을 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d80f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "val_loss = history_small_model.history[\"val_loss\"]\n",
    "epochs = range(1, 21)\n",
    "plt.plot(epochs, val_loss, \"b--\",\n",
    "        label=\"Validation loss\")\n",
    "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 과대적합할 수 없는 거서럼 보이다면 모델의 표현능력이 부족한 것,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be2d8e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3579 - accuracy: 0.8985 - val_loss: 0.1992 - val_accuracy: 0.9409\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1619 - accuracy: 0.9521 - val_loss: 0.1388 - val_accuracy: 0.9583\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1132 - accuracy: 0.9659 - val_loss: 0.1277 - val_accuracy: 0.9590\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0870 - accuracy: 0.9735 - val_loss: 0.1024 - val_accuracy: 0.9679\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0705 - accuracy: 0.9783 - val_loss: 0.0948 - val_accuracy: 0.9709\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0568 - accuracy: 0.9828 - val_loss: 0.0934 - val_accuracy: 0.9728\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0472 - accuracy: 0.9856 - val_loss: 0.0958 - val_accuracy: 0.9723\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0402 - accuracy: 0.9875 - val_loss: 0.0926 - val_accuracy: 0.9728\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0334 - accuracy: 0.9895 - val_loss: 0.1003 - val_accuracy: 0.9726\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0281 - accuracy: 0.9910 - val_loss: 0.1000 - val_accuracy: 0.9747\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0242 - accuracy: 0.9925 - val_loss: 0.0924 - val_accuracy: 0.9766\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0202 - accuracy: 0.9939 - val_loss: 0.1016 - val_accuracy: 0.9761\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0163 - accuracy: 0.9951 - val_loss: 0.1071 - val_accuracy: 0.9736\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.1150 - val_accuracy: 0.9746\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0127 - accuracy: 0.9962 - val_loss: 0.1128 - val_accuracy: 0.9754\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 0.1196 - val_accuracy: 0.9736\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.1257 - val_accuracy: 0.9743\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.1271 - val_accuracy: 0.9748\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.1290 - val_accuracy: 0.9736\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.1488 - val_accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "# 96개의 유닛을 가진 2개의 중간층으로 구성되어 용량이 더 큰 모델을 훈련해 보죠.\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(96, activation=\"relu\"),\n",
    "    layers.Dense(96, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=['accuracy'])\n",
    "history_large_model = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=20, batch_size=128,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b15068",
   "metadata": {},
   "source": [
    "## 5.4 일반화 성능 향상하기\n",
    "모델이 어느정도 일반화 성능을 갖고 과대적합할 수 있다면 이제 일반화를 극대화하는데 초점 <br>\n",
    "\n",
    "\n",
    "### 5.4.1 데이터셋 큐레이션\n",
    "- 데이터가 충분한지 확인 <br>\n",
    "- 레이블 할당 에러를 최소화한다 <br>\n",
    "- 데이터르 정제하고 누락된 값을 처리한다 <br>\n",
    "- 많은 특성중에서 어떤 것이 유용한지 확실하지 않다면 특성 서택을 할수 있ㄷ. <br>\n",
    "\n",
    "### 5.4.2 특성 공학(feature engineering)\n",
    "- 데이터와 머신 러닝 알고리즘에 관한 지식을 사용하는 단계 <br>\n",
    "- 딥러닝 이전에는 특성 공학이 머신 러닝 workflow에서 가장 중요한 부분, 전통적인 얕은 학습 방법의 알고리즘들은 스스로 유용한 특성을 학습할 만큼 충분히 넓은 가서 공간을 가지고 있지 않음, 따라서 알고리즘에 데이터를 표현하는 방식에 성공 여부가 달려 있음. <br>\n",
    "- 다행히 최신 딥러닝은 대부분 특성 공학이 필요하지 않다, 신경망이 자동으로 원본 데이터에서 유용한 특성을 추출할 수 있기 때문, 그렇다면 심층 신경망을 사용할떄는 특성 공학에 대해 신경쓰지 않아도 될까? No, Here's two reashons why. <br>\n",
    "    1. 좋은 특성은 적은 자원을 사용하여 문제를 더 멋지게 풀어낼 수 있다, 예를 들어 시계 바늘을 읽는 문제에 합성곱 신경망을 사용하는 것은 어울리지 않다. <br>\n",
    "    2. 좋은 특성은 더 적은 데이터로 문제를 풀 수 있다. 딥러닝 모델이 스스로 특성을 학습하는 능력은 가용한 훈련 데이터가 많을 떄 발휘된다. 샘플 개수가 적다면 특성에 있는 정보가 매우 중요해진다. <br>\n",
    "\n",
    "\n",
    "### 5.4.3 조기 종료 사용\n",
    "EarlyStopping 콜백을 사용\n",
    "\n",
    "\n",
    "### 5.4.4 모델 규제하기\n",
    "- 규제(regularization)기법은 훈련 데이터에 완벽하게 맞추려는 모델의 능력을 적극적으로 방해하는 일련의 모범 사례, 모델의 검증 점수를 향상시키는 것이 목적 <br>\n",
    "- 과대적합을 완화시키는 가장 간단한 방법은 모델크기(층의 수와 층에 있는 유닛개수로 결정되는 학습 가능한 파라미터 개수)를 줄이는 것, 너무 많은 용량과 충분하지 않은 용량 사이의 절충점을 찾아야함 <br>\n",
    "\n",
    "#### 코드 5-10 원본 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2da39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.5445 - accuracy: 0.5022"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), _ = imdb.load_data(num_words=10000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "train_data = vectorize_sequences(train_data)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"binary_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "history_original = model.fit(train_data, train_labels, epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f71828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더 작은 모델로 바꿔보쟈\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(4, activation='relu'),\n",
    "    layers.Dense(4, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history_smaller_model = model.fit(\n",
    "    train_data, train_labels, epochs=20, batch_size=512, validation_split=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1947084",
   "metadata": {},
   "source": [
    "- 4번째 epoch가 아니라 6번쨰 epoch 작은 모델이 기본 모델보다 더 나중에 과대적합되기 시작 <br>\n",
    "- 과대적합이 시작되었을 때 성능이 더 천천히 감소\n",
    "\n",
    "#### 코드 5-12 큰 용량의 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73705db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "history_larger_model = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4\n",
    ")\n",
    "\n",
    "# 용량이 큰 모델은 첫번째 에포크 이후 거의 바로 과대적합이 시작되어 갈수록 더 심해진다, 검증 손실도 매우 불안정함, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a62f2",
   "metadata": {},
   "source": [
    "#### 가중치 규제 추가\n",
    "\n",
    "'오캄의 면도날': 어떤 것에 대한 두가지의 설명이 있다면 더 적은 가정이 필요한 간단한 설명이 옳을 것이라는 이론 <br>\n",
    "\n",
    "가중치 규제(weight regularization)이라고 하며, 모델의 손실 함수에 큰 가중치에 연관된 비용을 추가 <br>\n",
    "- L1규제: 가중치의 절댓값에 비례하는 비용이 추가됨 (가중치의 L1노름(norm)) <br>\n",
    "- L2규제: 가중치의 제곱에 비례하는 비용이 추가됨(가중치의 L2노름(norm), L2규제는 신경망에서 가중치 감쇠(weight decay)라고도 부름. 이름이 다르지만 혼동하지 마셈, 가중치 감쇠는 수학적으로 L2규제와 동일 <br>\n",
    "\n",
    "#### 코드 5-13 모델에 L2 가중치 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51413b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, kernel_regularizer = regularizers.l2(0.02),\n",
    "                activation='relu'),\n",
    "    layers.Dense(16, kernel_regularizer = regularizers.l2(0.02),\n",
    "                activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "history_l2_reg = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split =0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc3af7",
   "metadata": {},
   "source": [
    "#### 코드 5-14 케라스에서 사용할 수 있는 가중치 규제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7cebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "regularizers.l1(0.001)  # ㅣ1규제\n",
    "regularizers.l1_l2(l1=0.001, l2 = 0.001)  # l1규제와 l2규제 병행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a193dccd",
   "metadata": {},
   "source": [
    "#### 드롭아웃 추가 \n",
    "\n",
    "모델층에 드롭아웃을 적용하면 훈련하는 동안 무작위로 층의 출력 특성ㅇ르 일부 제외시킴(0으로 만듦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ceab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output *= np.random.randint(0, high=2, size=layer_output.shape)  # 훈련할 때 유닛의 출력 중 50%를 버림\n",
    "layer_output *= 0.5  # 테스트 단계\n",
    "layer_output *= np.random.randint(0, high=2, size=layer_output.shape)  # 훈련 단계\n",
    "layer_output .= 0.5  # 여기에서 스케일을 낮추는 대신 높임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9242cf",
   "metadata": {},
   "source": [
    "- 핵심 아이디어는 층의 출력값에 노이즈를 추가하여 중요하지 않은 우연한 패턴(힌틴이 이야기한 부정한 협업)을 깨뜨리는 것임\n",
    "- 노이즈가 없다면 모델이 이 패턴을 기억하기 시작할 것\n",
    "\n",
    "#### 코드 5-15 IMDB 모델에 드롭아웃 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39953cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsporp',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "history_dropout = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size = 512, validation_split = 0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca4188",
   "metadata": {},
   "source": [
    "신경망에서 일반화 성능을 극대화하고 과대적합을 방지하기 위해 가장 널리 사용하는 방법은 다음과 같다.\n",
    "\n",
    "- 훈련 데이터를 더 모음, 또는 더 나은 데이터를 모음 <br>\n",
    "- 더 나은 특성을 개발 <br>\n",
    "- 네트워크의 용량을 감소 <br>\n",
    "- (작은 모델을 만들기 위해) 가중치 규제를 추가 <br>\n",
    "- 드롭아웃을 추가 <br>\n",
    "\n",
    "\n",
    "\n",
    "## 5.5 요약\n",
    "- 머신러닝 모델의 목적은 이전에 본 적 없는 입력에서 정확하게 동작하는 일반화 <br>\n",
    "- 심층 신경망은 훈련 샘플 사이를 성공적으로 보간 할 수 있는 모수 모델(parametric model)을 훈련하여 일반화를 달성, '잠재 매니폴드'를 학습해다고 말할 수 있음 <br>\n",
    "- 머신러닝의 근본적인 문제는 최적화와 일반화 사이의 줄다리기: 일반화를 달성하기 위해 먼저 훈련 데이터에 잘 맞추어야 하지만 훈련 데이터에 대한 성능 향상은 잠시 후 불가피하게 일반화를 저해함 <br>\n",
    "- 딥러닝 모델의 일반화 능력은 데이터의 잠재 매니폴드를 근사하는 방법을 학습하고 보간을 통해 새로운 입력을 이해할 수 있다는 사실에서 비롯됨 <br>\n",
    "- 모델을 개발하는 동안 모델의 일반화 능력을 정확하게 평가할 수 있어야 함. <br>\n",
    "- 모델을 구축하기 시작할 때 먼저 약간의 일반화 능력을 가지고 과대적합할 수 있느 모델을 만드는 것이 목표 <br>\n",
    "- 모델이 과대적합되기 시작할 떄 규제를 통해 일반화 성능을 향상시키도록 목표가 바뀜 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199b0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30539697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5beb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57eb540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016be26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0ec59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3893a5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d7db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a53f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f6f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a096e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
