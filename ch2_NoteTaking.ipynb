{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59270dd6",
   "metadata": {},
   "source": [
    "## 2 신경망의 수학적 구성요소\n",
    "\n",
    "2.1 신경망과의 첫만남 <br>\n",
    "2.2 신경망을 위한 데이터 표현 <br>\n",
    "2.3 신경망의 톱니바퀴: 텐서연산 <br>\n",
    "2.4 신경망의 엔진: 그레이디언트 기반 최적화 <br>\n",
    "2.5 첫번째 예제 다시 살펴보기 <br>\n",
    "2.6 요약 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60655b00",
   "metadata": {},
   "source": [
    "### 2.1 신경망과의 첫 만남\n",
    "#### 코드2-1 케라스에서 MNIST데이터셋 적재하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda9711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820aaba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "60000\n",
      "[5 0 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "# train데이터\n",
    "print(train_images.shape)\n",
    "print(len(train_labels))\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b98bb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n",
      "10000\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# test데이터\n",
    "print(test_images.shape)\n",
    "print(len(test_labels))\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554212f",
   "metadata": {},
   "source": [
    "#### 코드2-2 신경망 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f6edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(52, activation='relu'), # 층, layer\n",
    "    layers.Dense(10, activation='softmax') # softmax층\n",
    "])\n",
    "\n",
    "# 완전 연결(fully-connected)된 신경망층인 Dense층이 2개가 연속되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8b899",
   "metadata": {},
   "source": [
    "#### 코드 2-3 컴파일단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a70b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', # 성능향상, 모델을 업데이트 하는 메커니즘\n",
    "             loss='sparse_categorical_crossentropy', # 모델의 성능을 측정하는 방법으로 모델이 옳음 방향으로 학습될 수 있도록 도와줌\n",
    "              metrics=[\"accuracy\"]) # 여기서는 정확도(정확히 분류된 이미지의 비율)만 보겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2fd40",
   "metadata": {},
   "source": [
    "#### 코드 2-4 이미지 데이터 준비하기\n",
    "데이터를 모델에 맞는 크기로 바꾸고 모든 값을 0과 1사이로 스케일을 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbd7096",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype(\"float32\")/255\n",
    "test_images = test_images.reshape((10000, 28*28))\n",
    "test_images = test_images.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b65aa5",
   "metadata": {},
   "source": [
    "#### 코드 2-5 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac8b9d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.4025 - accuracy: 0.8911\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.2115 - accuracy: 0.9398\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1641 - accuracy: 0.9537\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1361 - accuracy: 0.9619\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1169 - accuracy: 0.9670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff8e3a9d9d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf7a1bd",
   "metadata": {},
   "source": [
    "#### 코드 2-6 모델을 사용하여 예측만들기\n",
    "\n",
    "데스트 세트처럼 훈련 데이터가 아닌 이미지를 지칭함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fda33e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0764012e-05, 8.7391907e-09, 5.0407900e-05, 4.0145321e-03,\n",
       "       9.6506243e-09, 3.5486621e-05, 1.7453468e-12, 9.9565518e-01,\n",
       "       1.7059710e-05, 2.1663107e-04], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_digits = test_images[0:10] \n",
    "predictions = model.predict(test_digits)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "638a543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "0.9956552\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0].argmax())\n",
    "print(predictions[0][7])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0aa5e",
   "metadata": {},
   "source": [
    "#### 코드 2-7 새로운 데이터에서 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4857c193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9657\n",
      "테스트 정확도: 0.9656999707221985\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"테스트 정확도: {test_acc}\")\n",
    "\n",
    "# 모델 훈련했을때 나는 정확도 84%가 나왔는데, 그것보단 지금 평가가 낮으니 내꺼는 훈련이 잘됐음\n",
    "# 교재에선 overfitting 됐음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8765f0ce",
   "metadata": {},
   "source": [
    "### 2.2 신경망을 위한 데이터 표현\n",
    "\n",
    "Tensor는 다차원 넘파이 배열에 데이터를 저장하는 것부터 시작 <br>\n",
    "텐서는 데이터를 위한 container <br>\n",
    "텐서는 임의의 차원 개수를 자기는 행렬의 일반화된 모습, 텐서에서는 차원(dimension)을 종종 축(axis) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a7acb",
   "metadata": {},
   "source": [
    "#### 2.2.1 스칼라(랭크-0 텐서)\n",
    "\n",
    "- 스칼라(scalar) = Scalar Tensor, Rank-0 Tensor, 0D Tensor <br>\n",
    "- 텐서의 축 개수를 랭크(rank) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "705146e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# ex)\n",
    "import numpy as np\n",
    "x = np.array(12)\n",
    "print(x)\n",
    "print(x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa5e3c3",
   "metadata": {},
   "source": [
    "#### 2.2.2 벡터(랭크-1 텐서)\n",
    "\n",
    "- 벡터(vector) = Rank-1 Tensor, 1D Tensor <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5128279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12  3  6 14  7]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# ex)\n",
    "x = np.array([12, 3, 6, 14, 7])\n",
    "print(x)\n",
    "print(x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f2b850",
   "metadata": {},
   "source": [
    "#### 2.2.3 행렬(랭크-2 텐서)\n",
    "\n",
    "- 행렬(matrix) = Rank-2 Tensor, 2D Tensor <br>\n",
    "- has 2 axes: row & column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65387754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# ex)\n",
    "x = np.array([[5, 78, 2, 34, 0], # 행 -> , 첫번째 행 [5, 78, 2, 34, 0]\n",
    "             [6, 79, 3, 35, 1],  # 열↓, 첫번째 열[5,6,7]\n",
    "             [7, 80, 4, 36, 2]])\n",
    "print(x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401420b6",
   "metadata": {},
   "source": [
    "#### 2.2.4 랭크-3 텐서와 더 높은 랭크의 텐서\n",
    "\n",
    "- 이런 행렬들을 하나의 새로운 배열로 합치면 숫자가 채워진 직육면체 형태로 해석할 수 있는 랭크-3 텐서 (또는 3D텐서)가 만들어진다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec036b39",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1715295208.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_101/1715295208.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    (10,).shape\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "(25, 3, 6, 5)\n",
    "train[batchsize*(n-1):batch_size*n\n",
    "(10,).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5f7f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(1, 3)\n",
      "[[ 2.  4.  6.]\n",
      " [ 5.  7.  9.]\n",
      " [ 8. 10. 12.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])\n",
    "Y = np.array([[1., 2., 3.]])\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X+Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60705031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(2,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,3) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_101/2550644214.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,3) (2,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])\n",
    "y = np.array([1., 2.])\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print((X+y))\n",
    "\n",
    "# 애러확인!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "598d5454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3,)\n",
      "tf.Tensor(\n",
      "[[ 2.  3.  5.]\n",
      " [ 5.  6.  8.]\n",
      " [ 8.  9. 11.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])\n",
    "# y = tf.constant([1., 1.])\n",
    "y = tf.constant([1., 1., 2.])\n",
    "\n",
    "print(x.get_shape())\n",
    "print(y.get_shape())\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30dbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc1adf58",
   "metadata": {},
   "source": [
    "#### 2.2.5 핵심 속성\n",
    "\n",
    "- 축의 개수(랭크): 랭크-3 텐서에서는 3개의 축이 있고, 행렬에는 2개의 축이 있다. <br>\n",
    "- 크기shape: 텐서의 각 축을 따라 얼마나 많은 차원이 있는지를 나타낸 파이썬의 튜플(tuple) <br>\n",
    "    - ex) 2.2.3의 예시는 (3,5), rank3Tensor: (3,3,5) <br>\n",
    "- 데이터 타입(dtype): 텐서의 각 요소의 데이터 타입 <br>\n",
    "    - ex) float16, float32, float64, unit8, 가끔 string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db327bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(train_images.ndim)\n",
    "print(train_images.shape)\n",
    "print(train_images.dtype)\n",
    "\n",
    "# 8비트 정수형 rank-3 tensor, \n",
    "# 28*28 크기의 정수 행렬 6만개, \n",
    "# 각 행렬은 하나의 흑백 이미지, 행렬의 각 원소는 0과 255사이의 값을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae9493",
   "metadata": {},
   "source": [
    "#### 코드 2-8 다섯번쨰 이미지 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2710a9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digit = train_images[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "print(train_labels[4]) # 정수 9!\n",
    "print(train_labels[4].dtype) # dtype은 unit8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f938efd7",
   "metadata": {},
   "source": [
    "#### 2.2.6 넘파이로 텐서 조작하기\n",
    "\n",
    "슬라이싱(slicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a378ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 28, 28)\n",
      "(90, 28, 28)\n",
      "(90, 28, 28)\n",
      "(60000, 14, 14)\n",
      "(60000, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "# ex) \n",
    "my_slice = train_images[10:100]\n",
    "print(my_slice.shape)\n",
    "\n",
    "my_slice2 = train_images[10:100, : : ] # 위에꺼랑 동일함\n",
    "print(my_slice2.shape)\n",
    "\n",
    "my_slice3 = train_images[10:100, 0:28, 0:28 ] # 이전과 동일\n",
    "print(my_slice3.shape)\n",
    "\n",
    "my_slice4 = train_images[:, 14:, 14:] \n",
    "print(my_slice4.shape)\n",
    "\n",
    "my_slice5 = train_images[:, 7:-7, 7:-7] \n",
    "print(my_slice5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f219a926",
   "metadata": {},
   "source": [
    "#### 2.2.7 배치 데이터\n",
    "\n",
    "- 첫번째 축(인덱스가 0부터 시작)은 샘플 축(sample axis) = sample dimension <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f9a127e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "# ex)\n",
    "batch1 = train_images[:128]\n",
    "batch2 = train_images[128:256]\n",
    "# print(batch1)\n",
    "# print(batch2)\n",
    "\n",
    "n=3\n",
    "batch3 = train_images[128*n:128*(n+1)]\n",
    "print(batch3)\n",
    "\n",
    "# 첫번쨰 축을 batch axis or batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38d6ba",
   "metadata": {},
   "source": [
    "#### 2.2.8 텐서의 실제 사례\n",
    "\n",
    "- 벡터 데이터: (samples, features) 크기의 랭크-2 텐서, 각 샘플은 수치 속성(특성, feature)로 구성된 벡터 <br>\n",
    "- 시계열 데이터 또는 시퀀스 (sequence) 데이터: (samples, timesteps,features) 크기의 랭크-3 텐서, 각 샘플은 특성 벡터의 길이가 timesteps인 sequence<br>\n",
    "- 이미지: (samples, height, width, channels) or (samples, channels, height, width) 크기의 랭크-4 텐서, 각 샘플은 픽셀의 2d 격자고 각 픽셀은 수치 값(채널(channel)의 벡터 <br>\n",
    "- 동영상: (samples, frames, height, width, channles) 또는 (samples, frames, channels, height, width) 크기의 랭크-5 텐서, 각 샘플은 이미지의 길이가 frames인 sequence <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28e84f",
   "metadata": {},
   "source": [
    "#### 2.2.9 벡터 데이터\n",
    "\n",
    "- 1번째 축: 샘플 축\n",
    "- 2번째 축: 특성 축feature axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a8bd0e",
   "metadata": {},
   "source": [
    "#### 2.2.10 시계열 데이터 또는 시퀀스 데이터\n",
    "\n",
    "2 examples) <br>\n",
    "- 주식 가격 데이터셋\n",
    "- twitter dataset: 100만개의 트윗은 (1000000, 280, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2363023",
   "metadata": {},
   "source": [
    "#### 2.2.11 이미지 데이터\n",
    "\n",
    "- 흑백: Rank2 Tensor, since only 1 channel => (128, 256, 256, 1) <br>\n",
    "- 컬러 이미지: Rank2 Tensor, 3 channels => (128, 256, 256, 3) <br><br>\n",
    "\n",
    "이미지 텐서의 크기를 지정하는 방식 2가지:\n",
    "- 채널-마지막(channel last): (samples, height, width, color_depth) <br> \n",
    "- 채널-우선(channel first): (samples, color_depth, height, width), ex: (128, 1, 256, 256), (128, 3, 256, 256) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b709a2",
   "metadata": {},
   "source": [
    "#### 2.2.12 비디오 데이터\n",
    "\n",
    "Rank-5 Tensor <br>\n",
    "- 1Frame: (height, width, color_depth)의 Rank3 Tensor <br>\n",
    "- Sequences of Frames: (frames, height, width, color_depth)의 Rank4 Tensor <br>\n",
    "- video batch: (samples, frames, height, width, color_depth)의 Rank5 Tensor <br>\n",
    "ex) 60초짜리 144*256 유튜브 클립 = 240 프레임 <br>\n",
    "(4, 240, 144, 256, 3) => total 106168320"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90378204",
   "metadata": {},
   "source": [
    "### 2.3 신경망의 톱니바퀴: 텐서연산\n",
    "\n",
    "Tensor operation = Tensor Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab51df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 층 생성 방식\n",
    "keras.layers.Dense(512, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3790c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W는 행렬, b는 벡터\n",
    "output = relu(dot(W, input)+b)\n",
    "\n",
    "# 입력 텐서와 텐서 W사이의 점곱(dot)\n",
    "# 점곱으로 만들어진 행렬과 벡터 b사이의 덧셈(+)\n",
    "# relu 연산. relu(x)는 max(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c1f526",
   "metadata": {},
   "source": [
    "#### 2.3.1 원소별 연산\n",
    "\n",
    "relu함수와 덧셈은 원소별 연산(element-wise operation), 각 원소에 독립적으로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex) relu연산 구현\n",
    "def naive_relu(x):\n",
    "    assert len(x.shape) ==2  # x는 rank-2 numpy array\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80bdd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex) 덧셈도 같음\n",
    "def naive_add(x, y):\n",
    "    assert len(x.shape) ==2  # x와 y는 rank-2 numpy array\n",
    "    assert x.shape == y.shape\n",
    "    x = x.copy()  # 입력 텐서 자체를 바꾸지 않도록 복사\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x\n",
    "\n",
    "# 곱셈, 뺄셈등도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532efd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "z = x+y   # 원소별 덧셈\n",
    "z = np.maximu(z, 0.) # 원소별 렐루 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e6b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x = np.random.random((20, 100))\n",
    "y = np.random.random((20, 100))\n",
    "\n",
    "t0 = time.time()\n",
    "for _ in range(1000):\n",
    "    z = x+y\n",
    "    z = np.maximum(z, 0.)\n",
    "print(\"걸린시간: {0:.2f} s\".format(time.time()- t0))  # 0.02초가 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfb58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "for _ in range(1000):\n",
    "    z = naive_add(x,y)\n",
    "    z = naive_relu(z)\n",
    "print(\"걸린시간: {0:.2f} s\".format(time.time()- t0)) \n",
    "\n",
    "# CUDA 구현을 통해 원소별 연산이 실행됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683ecba",
   "metadata": {},
   "source": [
    "#### 2.3.2 Broadcasting\n",
    "\n",
    "has 2 steps:\n",
    "1. 큰 텐서의 ndim에 맞도록 작은 텐서에 (broadcasting 축이라고 부르는) 축이 추가. <br>\n",
    "2. 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex)\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.random((32,10)) # X는 크기가(32,10)인 랜덤한 행렬\n",
    "y = np.random.random((10, )) # y는 크키가 (10,)인 랜덤한 행렬\n",
    "y = np.expand_dims(y, axis=0) # 이제 y의 크기는 (1, 10)\n",
    "Y = np.concatenate([y]*32, axis=0)  # 축 0을 따라 y를 32번 반혹하여 크기가 (32,10)인 Y를 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) ==2  # x는 Rank-2 Numpy Array\n",
    "    assert len(y.shape) ==1  # y는 Numpy Vector\n",
    "    assert x.shape[1] == y.shape[0] #\n",
    "    x = x.copy() # 입력 텐서 자체를 바꾸지 않도록 복사\n",
    "    for i in range(x.shape[0])\n",
    "        for j in range(x.shape[1])\n",
    "            x[i, j] += y[j]\n",
    "    return x\n",
    "\n",
    "# (a, b, ..., n, n+1, ...m) 크기의 텐서와 (n, n+1, ...m) 크기의 텐서 사이에 브로드캐스팅으로 원소별 연산을 적용가능\n",
    "# 이때 broadcasting은 a부터 n-1까지의 축에 자동으로 일어난다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d4c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원소별 maximum연산을 적용하는 예\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.random((64, 3, 32, 10)) # x는 (64, 3, 32,10) 크기의 랜덤 텐서\n",
    "y = np.random.random((32, 10)) # y는 (32,10) 크기의 랜덤 텐서\n",
    "z = np.maximum(x,y) # 출력 z크기는 x와 동일하게 (64, 3, 32,10)이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6638f327",
   "metadata": {},
   "source": [
    "#### 2.3.3 텐서 곱셈\n",
    "\n",
    "- 텐서 곱셈(tensor product) 또는 점곱(dot product) (* 연산자를 사용하는 원소별 곱셈과 혼동ㄴㄴ) <br>\n",
    "- 넘파이에서 텐서 곰셉은 np.dot 함수를 수행하여 수행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((32,))\n",
    "y = np.random.random((32,))\n",
    "z = np.dot(x, y)\n",
    "\n",
    "z = x•y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_vector_dot(x, y): \n",
    "    assert len(x.shape) == 1  \n",
    "    assert len(y.shape) == 1 # x와 y는 Numpy Vector\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    z = 0.\n",
    "    for i in ragne(x.shape[0]):\n",
    "        z += x[i]*y[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4f28557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2  # x는 Numpy Matrix\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]  # x의 두번째 차원이 y의 첫번째 차원과 같아야 한다\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬=벡터 점곱과 벡터-벡터 점곱 사으이 관계를 부각하기 위해 앞에서 만든 함수 재사용\n",
    "def native_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z\n",
    "\n",
    "# 두 텐서 중 하나라도 ndim이 1보다 크면 dot 연산에 교환 법칙이 성립되지 않는다. dot(x,y)는 dot(y,x) 같지 않다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2  \n",
    "    assert len(y.shape) == 2 # x와 y는 Numpy Matrix\n",
    "    assert x.shape[1] == y.shape[0]  # x의 두번째 차원이 y의 첫번째 차원과 같아야 한다\n",
    "    z = np.zeros((x.shape[0]), y.shape[1])  # 이 연산은 0이 채워진 특정 크기의 벡터를 만든다.\n",
    "    for i in range(x.shape[0]): # x의 행을 반복한다\n",
    "        for j in range(x.shape[1]): # y의 열을 반복한다\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y) \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0140cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, b, c, d) • (d,) => (a,b,c)\n",
    "(a, b, c, d) • (d,e) => (a,b,c,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e896d8",
   "metadata": {},
   "source": [
    "#### 2.3.3 텐서 크기 변환\n",
    "\n",
    "- 첫번째 신경망 예제의 Dense측에서는 사용되지 않지만 모델에 주입할 숫자 데이터를 전처리할 때 사용 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db005209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "[[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]]\n",
      "[[0. 1. 2.]\n",
      " [3. 4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.reshape((60000, 28*28)) \n",
    "# 텐서의 크기를 변환한다는 것은 특정 크기에 맞게 열과 행을 재배열\n",
    "# 원래 텐서와 원소 개수가 동일\n",
    "\n",
    "x = np.array([[0., 1.],\n",
    "             [2., 3.],\n",
    "             [4., 5.]])\n",
    "print(x.shape)\n",
    "\n",
    "x1 = x.reshape((6,1))\n",
    "print(x1)\n",
    "\n",
    "x2 = x.reshape((2,3))\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be4bb9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300)\n"
     ]
    }
   ],
   "source": [
    "# 자주 사용하는 건 전치행렬\n",
    "x3 = np.zeros((300, 20))\n",
    "x4 = np.transpose(x3)\n",
    "print(x4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e92878e",
   "metadata": {},
   "source": [
    "#### 2.3.5 텐서 여산의 기하학적 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88341018",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [0.5, 1]\n",
    "\n",
    "# 이동\n",
    "A + [1,1]\n",
    "\n",
    "# 회전\n",
    "R = [[cos(theta), -sin(theta)],\n",
    "     [sin(theta), cos(theta)]\n",
    "\n",
    "# 크기변경(scaling)\n",
    "S = [[horizontl_factor, 0], \n",
    "     [0, vertical_factor]]\n",
    "# 선형변환 (linear transform): 임의의 행렬과 점곱\n",
    "# 아핀변환 (affine transform) : 선형변환과 이동의 조합, y= W*x+b\n",
    "# relu 활성화 함수를 사용하는 Dense층: \n",
    "affine2(affine1(x)) = W2•(W1•x+b1) + b2 = (W2•W1)•x + (W2•b1+b2) \n",
    "# 선형변환 부분이 W2•W1, 이동부분이 W2•b1+b2\n",
    "     \n",
    "# 기울이기 (skewing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26953f3",
   "metadata": {},
   "source": [
    "#### 2.3.6 딥러닝의 기하학적 해석\n",
    "\n",
    "- 종이공을 펼치는 일 <br>\n",
    "- 기초적인 연산을 길게 연결하여 복잡한 기하학적 변환을 조금씩 분해하는 방식이 마치 사람이 종이 공을 펼치기 위한 전략과 비슷 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4cb14a",
   "metadata": {},
   "source": [
    "### 2.4 신경망으 엔진:gradient 기반 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f69ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output= relu(dot(W, input) + b) \n",
    "#  W and b = Weight(가중치) trainable parameter(훈련되는 파라미터)\n",
    "# W = Kernel \n",
    "# b = bias "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128c3ab",
   "metadata": {},
   "source": [
    "훈련(train)은 훈련반복루프(training loop) 안에서 일어남\n",
    "1. 훈련 샘플 x와 이에 상응하는 타깃 y_true의 배치를 추출 <br>\n",
    "2. x를 사용하여 모델을 실행하고(정방향 패스(forward pass) 단계), 예측 y_pred를 구한다. <br>\n",
    "3. y_pred와 y_true의 차이를 측정하여 이 배치에 대한 모데르이 손실을 계산 <br>\n",
    "4. 배치에 대한 손실이 조금 감소되도록 모델의 모든 가중치를 업데이트 <br><br> \n",
    "\n",
    "경사하강법(gradient descent)\n",
    "- 신겸앙을 가능하게 만든 최적화\n",
    "ex) z = x+y에서 y를 조금 변경하면 z가 조금 변경, y의 변경 방향을 알고있다면 z도 예측 가능\n",
    "\n",
    "미분가능(Differentiable)\n",
    "=> 모델의 가중치를 조금 변경하면 소실값이 예측 가능한 방향으로 조금 바뀜\n",
    "\n",
    "Gradient라는 수학 연산ㅇ르 사용하여 모델 가중치를 여러 방향으로 이동했을 때 손실이 얼마나 변하는지 설명 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf41d2f",
   "metadata": {},
   "source": [
    "#### 2.4.1 도함수란?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex)\n",
    "f(x) = y\n",
    "f(x+epsilon_x) = y + a*epsilon_x\n",
    "# 이 선형적인 근사는 x가 p에 충분히 가까울 떄 유효\n",
    "# 기울기 p에서 f의 도함수(derivative)라고 함\n",
    "\n",
    "# 왼쪽의 도함수는 오른쪽\n",
    "cos(x) = -sin(x) \n",
    "\n",
    "f(x) = a*x = f'(x)=a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffe4525",
   "metadata": {},
   "source": [
    "#### 2.4.2 텐서 연산의 도함수: Gradient\n",
    "\n",
    "- 텐서연산(or 텐서함수)의 도함수를 Gradient라고 부름, 이를 곡률(curve)로 나타냄 <br>\n",
    "- 머신러닝의 예 <br>\n",
    "    - 입력 벡터, x (데이터셋에 있는 샘플) <br>\n",
    "    - 행렬, W (모델의 가중치) <br>\n",
    "    - 타깃, y_true(모델이 x에 연관시키기 위해 학습해ㅑ할 값) <br> \n",
    "    - 손실 함수, loss(현재의 예측과 y_true간의 차이를 측정하기 위해 사용) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ba720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W를 사용하여 y_pred를 계싼하고 그다으 예측 y_pred와 타깃 y_true사이의 손실 또는 차이를 계산\n",
    "y_pred = dot(W, x) # 모델 가중치 W를 사용하여 x에 대한 예측을 만든다\n",
    "loss_value = loss(y_pred, y_true) # 예측이 얼마나 벗어났는지 추정\n",
    "\n",
    "loss_value = f(W) # f는 W가 변화할 때 손실 값이 형성하는 곡선(또는 다차원 표면)을 설명\n",
    "\n",
    "# 현재의 W값을 W0이라고 할때, 점 W0에서 f의 도함수는 W와 크기가 같은 텐서 grad(loss_value, W0)\n",
    "# 이 텐서의 각 원소 grad(loss_value, W0[)i,j]는 W0[i,j]를 수정했을 떄 loss_value가 바뀌는 방향과 크기를 나타낸다.\n",
    "# 텐서 grad(loss_value, W0)가 W0에서 함수 f(W) = loss_value의 gradient\n",
    "# W0근처에서 W에 대한 loss_value의 gradient'라고 말한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9353ded",
   "metadata": {},
   "source": [
    "편도 함수 <br>\n",
    "(입력으로 행렬W를 받는) 텐서 연산 grad(f(W), W)는 스칼라 함수 grad_ij(f(W), w_ij)의 조합으로 표현할 수 있다. 이 스칼라 함수는 W의 다른 모든 가중치가 일정하다고 가정할 떄 가중치 W[i,j]에 대한 loss_value = f(W)의 도함수를 반환하다. 이떄 grad_ij를 W[i, j]에 대한 f의 편도함수(partial derivative)라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d3dd3",
   "metadata": {},
   "source": [
    "#### 2.4.3 확률적 경사하강법 (SGD, StochasticGradientDescent)\n",
    "\n",
    "함수의 최솟값은 도함수가 0인 지점, 따라서 이 지점을 모두 찾고 이 중에서 어떤 포인트의 함수 값이 가장 작은지 확인하는 것 <br>\n",
    "\n",
    "1. 훈련 샘플 배치 x와 이에 상응하는 타깃 y_true를 추출 <br>\n",
    "2. x로 모델을 실행하고 예측 y_pred를 구함(이를 정방향 패스라고 한다) <br>\n",
    "3. 이 배치에서 y_pred와 y_true 사이의 오차를 측정하여 모델의 손실을 계산 <br>\n",
    "4. 모델의 파라미터에 대한 손실 함수의 gradient를 계산 (이를 역방향패스(backward pass)라고 한다 <br>\n",
    "5. gradient의 반대 방향으올 파라미터를 조금 이동시킨다. 예를 들어 W-= learning_rate * gradient처럼 하면 배치에 대한 손실이 조금 감소함. learning_rate은 경사 하강법 과정의 속도를 조절하는 스칼라 값 <br>\n",
    "\n",
    "위에 5번이 mini-batch stochastic gradient descent (SGD), 확률적(stochastic)이란 단어는 각 배치 데이터가 무작위로 선택된다는 의미(확률적이란 것은 무작위(random)하다는 것의 과학적 표현). <br>\n",
    "하지만 경사하강법을 2차원에 표현하는 것이 쉽지않음 => 시렞론 1000000이상 차원의 공간을 2d 저차원으로 표현하는게 실전과 항상 맞지는 않는다! <br>\n",
    "\n",
    "최적화(=optimization or optimizer)의 변종<br>\n",
    "- SGD\n",
    "- Adagrad\n",
    "- RMSProp \n",
    "\n",
    "비교점<br>\n",
    "- 확률적경사하강법(StochasticGradientDescent): 하나의 샘플과 하나의 타깃을 뽑는것\n",
    "- mini-batch SGD: 둘의 절충안 \n",
    "- 배치경사하강법(Gradient Descent): 모든 데이터를 사용하여 반복을 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf778ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_velocity = 0\n",
    "momentum = 0.1  # 모멘텀 상수\n",
    "while loss>0.01:  # 최적화 반복 루프\n",
    "    w, loss, gradient = get_current_parameters()\n",
    "    velocity = momentum*past_velocity - learning_rate*gradient\n",
    "    x = w+momentum*velocity - learning_rate*gradient \n",
    "    past_velocity = velocity\n",
    "    update_paremeter(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3a542",
   "metadata": {},
   "source": [
    "#### 2.4.4 도함수 연결: 역전파 알고리즘\n",
    "\n",
    "함수가 미분 가능하기 때문에 gradient를 쉽게 계산할 수 있다고 가정했음 과연 really? <br>\n",
    "2개 이상의 층을 가진 모델의 경우 가중치에 대한 손실의 gradient를 어떻게 구함? 이게 역전파 알고리즘(Backpropagation Algorithm)이 필요한 이유<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25be4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연쇄법칙\n",
    "loss_value = loss(y_true, softmax(dot(relu(dot(inputs, W1) +b1), W2) + b2))\n",
    "\n",
    "# 미적분의 연쇄법칙(chain rule)을 사용하면 이렇게 연결된 함수의 도함수르 ㄹ구할 수 있따. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb209ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fg(x):\n",
    "    x1 = g(x)\n",
    "    y = f(x1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0536e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(y, x) == grad(y, x1) * grad(x1, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ea9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fghj(x):\n",
    "    x1 = j(x)\n",
    "    x2 = h(x1)\n",
    "    x3 = g(x2)\n",
    "    y = f(x3)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb630c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(y, x) == (grad(y, x3) * grad(x3, x2) * grad(x2, x1) * grad(x1, x)) \n",
    "# 이 연쇄법칙을 적용하는 것이 역전파 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91dc77b",
   "metadata": {},
   "source": [
    "<b> 계산 그래프를 활용한 자동 미분 (전부 다 그림이라 이 부분은 간단하게 summary하겠음) </b> <br><br>\n",
    "\n",
    "- 정방향패스: 위에어 아래로 그래프의 모든 노드에 전파 <-> 역방향패스: 그 반대 <br>\n",
    "- 나올수 있는 결과: <br>\n",
    "    - grad(loss_val, x2) = 1, x2가 epsilon만큼 변할때 loss_val = abs(4-x2) <br>\n",
    "    - grad(x2, x1) = 1, x1가 epsilon만큼 변할때 x2 = x1+b = 6+b <br>\n",
    "    - grad(x1, w) = 2, w가 epsilon만큼 변할때 x1 = x * w = 2 * w는 2 * epsilon만큼 변하기 때문 <br><br>\n",
    "    \n",
    "- 연쇄법칙이 역방향 그래프에 대해 알려주는 것: 노드가 연겨된 경로를 따라 각 에지의 도함수를 곱하면 어떤 노드에 대한 다른 노드이 도함수를 얻을 수 있음 <br>\n",
    "    - grad(loss_val, w) = grad(loss_val, x2) * grad(x2, x1) * grad(x1, w) <br>\n",
    "    - grad(loss_val, w) = 1*1*2 = 2 <br>\n",
    "    - grad(loss_val, b) = 1*1 = 1 <br>\n",
    "- 텐서플로우는 자동미분이 가능 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c81a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로의 gradientTape\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(0.)  # 초깃값 0으로 스칼라 변수를 생성\n",
    "with tf.GradientTape() as tape: # GradientTape블록을 시작\n",
    "    y = 2*x +3  # 이 블록안에엇 변수에 텐서 연산을 적용\n",
    "grad_of_y_wrt_x = tape.gradient(y, w) # tape를 사용해서 변수 x에 대한 출력 y의 gradient를 계산\n",
    "\n",
    "# GradientTape을 다차원 텐서와 함꼐 사용할 수 있음\n",
    "x = tf.Variable(tf.zeros(2,2))\n",
    "with tf.GradientTape() as tape:\n",
    "    y = 2*x +3 \n",
    "grad_of_y_wrt_x = tape.gradient(y, x) # grad_of_y_wrt_x는 (x와 크기가 같은) (2,2) 크기의 텐서로 x=[[0,0], [0,0]]일때 y=2*x+3의 곡률을 나타냄\n",
    "\n",
    "W = tf.Variable(tf.random.uniform((2,2)))\n",
    "b = tf.Variable(tf.zeros((2,)))\n",
    "x = tf.random.unifor((2,2))\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.matmul(x, W) + b\n",
    "grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b]) # grad_of_y_wrt_W_and_b는 2개의 텐서를 담은 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bbe57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96614dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30edfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a947a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da560e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048bb824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950a5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0d769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784285c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216342f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57d663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385e5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425064ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e6caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ba2b862",
   "metadata": {},
   "source": [
    "### 2.5 첫번째 예시 다시 살펴보기\n",
    "\n",
    "1. 층이 서로 연결되어 모델을 구성, 모델은 입력 데이터를 예측으로 매핑 <br>\n",
    "2. 손실 함수가 이 예측과 타깃을 비교하여 손실 값을 만든다 <br>\n",
    "3. 모델의 예측이 기대한 것에 얼만나 잘 맞는지 측정한다 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b3f4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28*28)) \n",
    "train_images = train_images.astype(\"float32\")/255\n",
    "test_images = test_images.reshape((10000, 28*28)) \n",
    "test_images = test_images.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77d25bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63bed791",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss =\"sparse_categorical_crossentropy\",\n",
    "             metrics =[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffd6fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0279 - accuracy: 0.9919\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0213 - accuracy: 0.9939\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0164 - accuracy: 0.9952\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0124 - accuracy: 0.9965\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0097 - accuracy: 0.9972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff8c16c44c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "# 미적분의 연쇄 법칙에서 파생된 역전파 알고리즘을 사용하여 각 배치에서 모델이 ㄱ자ㅜㅇ치에 대한 손실의 gradient를 계산\n",
    "# 이 배치에서 손실 값을 감소시키는 방향으로 가중치를 이동"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d69885",
   "metadata": {},
   "source": [
    "#### 2.5.1 텐서플로를 사용하여 첫번째 예제를 밑바닥부터 다시 구현하기\n",
    "\n",
    "#### 단순한 Dense 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de10d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = activation(dot(W, input)+b)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 2개의 tensorflow 변수 W와 b를 만듦\n",
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "        w_shape = (input_size, output_size)  # 랜덤한 값으로 초기화된 (input_size, output_size) 크기의 행렬 W를 만든다.\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "        \n",
    "        b_shape = (output_size,) # 0으로 초기화된 (output_size,)크기의 벡터 b를 만든다.\n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "    \n",
    "    def __call__(self, inputs): # 정방향 패스를 수행한다\n",
    "        return self.activation(tf.matmul(inputs, self.W)+ self.b)\n",
    "    \n",
    "    @property\n",
    "    def weights(self): # 층의 가중치를 추출하기 위한 메서드\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127f0860",
   "metadata": {},
   "source": [
    "#### 단순한 Sequential 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391aeaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "        return weights\n",
    "    \n",
    "    model = NaiveSequential([\n",
    "        NaiveDense(input_size=28*28, output_size=512, activation=tf.nn.relu),\n",
    "        NaiveDense(input_size=28*28, output_size=10, activation=tf.nn.softmax),\n",
    "    ])\n",
    "    \n",
    "    assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdacc273",
   "metadata": {},
   "source": [
    "#### 배치 제너레이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9658076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images)/batch_size)\n",
    "    def next(self):\n",
    "        images = self.images[self.index:self.index+self.batch_size]\n",
    "        labels = self.labels[self.index:self.index+self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a172f57d",
   "metadata": {},
   "source": [
    "#### 2.5.2 훈련 스텍 실행하기\n",
    "\n",
    "1. 배치에 있는 이미지에 대해 모델의 예츠을 계산 <br>\n",
    "2. 실제 레이블을 사용하여 이 예측의 손실 값을 계산 <br>\n",
    "3. 모델 가중치에 대한 손실의 gradient를 계산 <br>\n",
    "4. 이 gradient의 반대 방향으로 가중치를 조금 이동 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f64cf705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.4절에서 사용한 GradientTape객체를 사용\n",
    "\n",
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch) # 정방향 패스를 실행한다(GradientTape)블록안에서 모델의 예측을 계산\n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical(\n",
    "            labels_batch, predictions\n",
    "        )\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "    gradients = tape.gradient(average_loss, model.weights)  # 가중치에 대한 손실의 gradient를 계산한다, gradients 리스트의 각 항목은 model.weights리스트에 있는 가중치에 매칭됨\n",
    "    update_weights(gradients, model.weights) # 이 gradient를 사용하여 가중치를 업데이트 함, \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5c67105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_weights함수를 구현하는 가장 간단한 방법은 각 가중치에서 gradient*learning_rate을 빼는 것\n",
    "learning_rate = 1e-3\n",
    "def update_weights(gradients, weights):\n",
    "    for g, w in zip(gradients, weights):\n",
    "        w.assign_sub(g*learning_rate)  # 텐서플로 변수의 assign_su 메서드는 -=와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35c953a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2bbca1",
   "metadata": {},
   "source": [
    "#### 2.5.3 전체 훈련 루프\n",
    "\n",
    "훈련 epoch 하나는 단순히 훈련 데이터의 각 배치에 대한 훈련 스텝을 반복하는 것, 전체 훈련 루프(loop)는 단순히 에포크의 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20ce116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"에포크 {epoch_counter}\")\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if batch_counter % 100 == 0:\n",
    "                print(f\"{batch_counter}번째 배치 손실: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21697f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype(\"float32\")/255\n",
    "test_images = test_images.reshape((10000, 28*28))\n",
    "test_images = test_images.astype(\"float32\")/255\n",
    "\n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba33b838",
   "metadata": {},
   "source": [
    "#### 2.5.4 모델 평가하기\n",
    "\n",
    "테스트 이미지에 대한 예측에 argmax함수를 적용하고, 예상 레이블과 비교하여 모델을 평가할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(test_images)\n",
    "predictions = predictions.numpy()   # 텐서플로 텐서의 .numpy() 메서드를 호출하여 넘파이 배열로 바꿈\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "matches = predicted_labels == test_labels\n",
    "print(f\"정확도: {matches.mean(): .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a75ced",
   "metadata": {},
   "source": [
    "#### 2.6 요약\n",
    "\n",
    "- 텐서는 현대 머신러닝 시스템으의 기초, 텐서는 dtype, ndim, shape의 속성을 제공 <br>\n",
    "- 텐서 연산(덧셈, 텐서 곱셈, 원소별 곱셈등)을 통해 수치 텐서를 조작가능 <br>\n",
    "- 딥러닝 모델은 가중치 텐서를 매개변수로 받느 간단한 텐서 연산을 연결하여 구성 <br>\n",
    "- 학습(learnin)은 훈련 데이터 샘플과 그에 상응하는 타깃이 주어졌을 때 손실 함수를 최소화 하는 모델의 가중치 값을 찾는 것을 의미 <br>\n",
    "- 데이터 샘플과 타깃의 배치를 랜덤하게 뽑고 이 배치에서 모델 파라미터에 대한 손실의 gradient를 계산함으로써 학습이 진행, 모델의 파라미터는 gradient의 반대 방향으로 조금씩(학습률에 의해 정의된 크기만큼) 움직임, 이를 mini-batch gradient descent이라고 부름 <br>\n",
    "- 전체 학습 과정은 신경망에 있는 모든 텐서 연산이 미분 가능하기 때문에 가능. 따라서현재 파라미터와 배치 데이터를 gradient값에 매핑해 주는 gradient함수를 구성하기 위해 미분의 연쇄법칙을 사용할 수 있다. We call this backpropagation\n",
    "- 뒤에선 손실과 옵티마이저에 대햐여.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16dd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cbb8c7d",
   "metadata": {},
   "source": [
    "#### 예상문제\n",
    "1. (128, 256, 256, 3) shape과 (3, 50)의 점곱(dot) = (128, 256, 256, 50) <br>\n",
    "\n",
    "2. (32, 10)shape의 텐서와 (10,)shape의 텐서가 덧셈연산가능? 뭐때문에 가능? 최종 shape? <br>\n",
    "가능, (32,10)으로 나옴, 브로드캐스팅연산때문에 가능 <br>\n",
    "\n",
    "\n",
    "3. SGD는 데이터 하나로 학습하고 역전파 진행, 이렇게 되면 수렴하는데 불안정 <br>\n",
    "배치경사는 데이터 전체를 넣고 학습, 속도가 느려짐 <br>\n",
    "절충안: 미니배치 SGD, 배치를 작은 단위로 쪼개서 학습 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b3666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab16af32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b374b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
